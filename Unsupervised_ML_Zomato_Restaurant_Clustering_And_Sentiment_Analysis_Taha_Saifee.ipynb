{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "mDgbUHAGgjLW",
        "O_i_v8NEhb9l",
        "HhfV-JJviCcP",
        "Y3lxredqlCYt",
        "3RnN4peoiCZX",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "JlHwYmJAmNHm",
        "35m5QtbWiB9F",
        "PoPl-ycgm1ru",
        "H0kj-8xxnORC",
        "nA9Y7ga8ng1Z",
        "PBTbrJXOngz2",
        "u3PMJOP6ngxN",
        "dauF4eBmngu3",
        "bKJF3rekwFvQ",
        "MSa1f5Uengrz",
        "GF8Ens_Soomf",
        "0wOQAZs5pc--",
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "KSlN3yHqYklG",
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "EM7whBJCYoAo",
        "fge-S5ZAYoAp",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "4Of9eVA-YrdM",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "bamQiAODYuh1",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "OH-pJp9IphqM",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "PIIx-8_IphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "b0JNsNcRphqO",
        "BZR9WyysphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "YJ55k-q6phqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "U2RJ9gkRphqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "JMzcOPDDphqR",
        "x-EpHcCOp1ci",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "n3dbpmDWp1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "Ag9LCva-p1cl",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "NC_X3p0fY2L0",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "g-ATYxFrGrvw",
        "Yfr_Vlr8HBkt",
        "8yEUt7NnHlrM",
        "tEA2Xm5dHt1r",
        "I79__PHVH19G",
        "Ou-I18pAyIpj",
        "fF3858GYyt-u",
        "4_0_7-oCpUZd",
        "hwyV_J3ipUZe",
        "3yB-zSqbpUZe",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "bn_IUdTipZyH",
        "49K5P_iCpZyH",
        "Nff-vKELpZyI",
        "kLW572S8pZyI",
        "dWbDXHzopZyI",
        "yLjJCtPM0KBk",
        "xiyOF9F70UgQ",
        "7wuGOrhz0itI",
        "id1riN9m0vUs",
        "578E2V7j08f6",
        "89xtkJwZ18nB",
        "67NQN5KX2AMe",
        "Iwf50b-R2tYG",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "-oLEiFgy-5Pf",
        "C74aWNz2AliB",
        "2DejudWSA-a0",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "TNVZ9zx19K6k",
        "nqoHp30x9hH9",
        "rMDnDkt2B6du",
        "yiiVWRdJDDil",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "T5CmagL3EC8N",
        "BhH2vgX9EjGr",
        "qjKvONjwE8ra",
        "P1XJ9OREExlT",
        "VFOzZv6IFROw",
        "TIqpNgepFxVj",
        "VfCC591jGiD4",
        "OB4l2ZhMeS1U",
        "ArJBuiUVfxKd",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tahascommit/Clustering-and-analyzing-sentiments-for-restaurants-on-Zomato./blob/main/Unsupervised_ML_Zomato_Restaurant_Clustering_And_Sentiment_Analysis_Taha_Saifee.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - **Unsupervised ML - Zomato Restaurant Clustering And Sentiment Analysis**\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Unsupervised\n",
        "##### **Contribution**    - Individual\n",
        "##### **Name** : Taha Saifee\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Objectives:\n",
        "\n",
        "1.  **Cluster Zomato Restaurants**: Group restaurants into distinct clusters.\n",
        "2.  **Sentiment Analysis of Reviews**: Analyze customer reviews to determine positive or negative sentiments, providing insights into customer satisfaction and preferences.\n",
        "\n",
        "#### Business Context:\n",
        "\n",
        "*   **Background**: Zomato, an Indian restaurant aggregator and food delivery service, has seen an evolution in the restaurant business in India. The growing number of restaurants and their diversity calls for an insightful analysis of this sector.\n",
        "*   **Relevance**: This project is crucial for understanding customer preferences, improving service quality, and enhancing the dining experience in India's dynamic food industry.\n",
        "\n",
        "#### Data Utilization:\n",
        "\n",
        "*   **Data Sources**: Restaurant data from Zomato.\n",
        "*   **Sentiment Analysis**: Customer reviews will be analyzed to extract sentiment polarity (positive or negative).\n",
        "\n",
        "#### Methodology:\n",
        "\n",
        "1.  **Data Preprocessing**: Cleaning and structuring data for analysis.\n",
        "2.  **Clustering Technique**: Utilizing unsupervised machine learning algorithms (like K-means, hierarchical clustering, DBSCAN) to group restaurants.\n",
        "3.  **Sentiment Analysis**: Employing NLP techniques to analyze and categorize the sentiments of user reviews.\n",
        "\n",
        "#### Outcomes:\n",
        "\n",
        "*   **Visualizations**: Graphical representations of clusters and sentiment analysis results for easier interpretation and decision-making.\n",
        "*   **Insights for Customers**: Assisting customers in finding the best-suited restaurants based on their preferences.\n",
        "*   **Business Strategy for Zomato**: Identification of areas for improvement, popular cuisines, and customer preferences to strategize business growth and customer satisfaction.\n",
        "\n",
        "#### Impact:\n",
        "\n",
        "*   **For Customers**: Enhanced decision-making in choosing restaurants aligning with their preferences.\n",
        "*   **For Zomato**: Strategic insights into customer preferences, leading to better service offerings and targeted marketing.\n",
        "\n",
        "#### Applications:\n",
        "\n",
        "*   **Customer Segmentation**: Identifying different customer segments for targeted marketing.\n",
        "*   **Market Analysis**: Understanding popular trends in cuisines and dining experiences.\n",
        "*   **Operational Improvement**: Identifying gaps in service and areas for improvement.\n",
        "*   **Critic Analysis**: Using reviewer metadata to identify influential critics in the industry."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your GitHub Link here."
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the evolving Indian restaurant landscape, Zomato faces the challenge of effectively categorizing an increasing number of diverse dining establishments and understanding customer sentiments towards them. This project seeks to address these challenges by employing unsupervised machine learning techniques for clustering restaurants and conducting sentiment analysis of customer reviews. The aim is to gain insights into customer preferences and dining trends, which will enable Zomato to enhance its service offerings, improve customer satisfaction, and make informed business decisions in the competitive food industry. The outcome of this project will not only benefit Zomato in optimizing its business strategy but also assist customers in making better-informed dining choices."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install contractions\n",
        "\n",
        "import contractions\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud\n",
        "from scipy import stats, sparse as sp\n",
        "import statsmodels.api as sm\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans, DBSCAN\n",
        "from sklearn.metrics import silhouette_score, classification_report, accuracy_score, confusion_matrix\n",
        "from sklearn.decomposition import PCA\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "import joblib\n",
        "from joblib import dump, load\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "import re\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "import xgboost as xgb\n",
        "from collections import Counter\n",
        "\n",
        "# NLTK Downloads\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "\n",
        "# Uploading via github repository\n",
        "metadata_df = pd.read_csv('https://raw.githubusercontent.com/Tahascommit/Clustering-and-analyzing-sentiments-for-restaurants-on-Zomato./main/Zomato%20Restaurant%20names%20and%20Metadata.csv')\n",
        "\n",
        "# Uploading via github repository\n",
        "reviews_df = pd.read_csv('https://raw.githubusercontent.com/Tahascommit/Clustering-and-analyzing-sentiments-for-restaurants-on-Zomato./main/Zomato%20Restaurant%20reviews.csv')\n"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "\n",
        "metadata_df.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "\n",
        "reviews_df.head()"
      ],
      "metadata": {
        "id": "6iKXwe1QlyLM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "\n",
        "metadata_df.shape"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Metadata Dataset: 105 rows and 6 columns."
      ],
      "metadata": {
        "id": "vvSnxc0LnarY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "\n",
        "reviews_df.shape"
      ],
      "metadata": {
        "id": "kwufxuOfmA-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reviews Dataset: 10,000 rows and 7 columns."
      ],
      "metadata": {
        "id": "_91cv2lNnd7K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "\n",
        "metadata_df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Metadata Dataset: All columns are of object type. 'Collections' has significant missing values."
      ],
      "metadata": {
        "id": "m6NsPQI4noxw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "\n",
        "reviews_df.info()"
      ],
      "metadata": {
        "id": "K0_llQhmmVL6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reviews Dataset: Mostly object types with one integer column ('Pictures'). There are some missing values in 'Reviewer', 'Review', 'Rating', 'Metadata', and 'Time'."
      ],
      "metadata": {
        "id": "5941HB19ntFg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "\n",
        "metadata_df.duplicated().sum()"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Metadata Dataset: No duplicates."
      ],
      "metadata": {
        "id": "2v1tLT8fn1UT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "\n",
        "reviews_df.duplicated().sum()"
      ],
      "metadata": {
        "id": "fanDmYIJmmjw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reviews Dataset: 36 duplicates."
      ],
      "metadata": {
        "id": "lotRBAP8n5Ha"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "\n",
        "metadata_df.isnull().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Metadata Dataset: Missing values in 'Collections' and one in 'Timings'."
      ],
      "metadata": {
        "id": "NjkjIiX6n9z6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "\n",
        "reviews_df.isnull().sum()"
      ],
      "metadata": {
        "id": "w1zm8q6tm1ro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reviews Dataset: Missing values in several columns, notably 'Review' and 'Reviewer'."
      ],
      "metadata": {
        "id": "GJzBl6jBoDcX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.heatmap(metadata_df.isnull(), cbar=False, cmap='viridis')\n",
        "plt.title('Missing Values in Metadata Dataset')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.heatmap(reviews_df.isnull(), cbar=False, cmap='viridis')\n",
        "plt.title('Missing Values in Reviews Dataset')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The visualizations above illustrate the distribution of missing values in both datasets:\n",
        "\n",
        "**Metadata Dataset:**\n",
        "Significant missing values in the 'Collections' column.\n",
        "A single missing value in the 'Timings' column.\n",
        "\n",
        "**Reviews Dataset:**\n",
        "Sparse missing values across the columns 'Reviewer', 'Review', 'Rating', 'Metadata', and 'Time'."
      ],
      "metadata": {
        "id": "BoVYmYV3oKOv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the analysis:\n",
        "\n",
        "The **metadata dataset** provides a comprehensive view of each restaurant, including its name, link, cost, collections, cuisines, and timings. The 'Collections' column has a considerable amount of missing data, which might impact analyses that rely heavily on this attribute.\n",
        "\n",
        "The **reviews dataset** contains detailed reviews for various restaurants, including the reviewer's name, review text, rating, metadata about the review, time of the review, and number of pictures included. The dataset is large, which is beneficial for sentiment analysis and understanding customer perspectives. However, the presence of missing values, particularly in the 'Review' and 'Reviewer' columns, needs to be addressed as these are key elements for sentiment analysis and reviewer profiling.\n",
        "\n"
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "\n",
        "metadata_df.columns.tolist()"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Metadata Dataset Columns:\n",
        "\n",
        "Name\n",
        "\n",
        "Links\n",
        "\n",
        "Cost\n",
        "\n",
        "Collections\n",
        "\n",
        "Cuisines\n",
        "\n",
        "Timings\n"
      ],
      "metadata": {
        "id": "F3MA03lIro-b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "\n",
        "reviews_df.columns.tolist()"
      ],
      "metadata": {
        "id": "PV7TatiBrQaV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reviews Dataset Columns:\n",
        "\n",
        "Restaurant\n",
        "\n",
        "Reviewer\n",
        "\n",
        "Review\n",
        "\n",
        "Rating\n",
        "\n",
        "Metadata\n",
        "\n",
        "Time\n",
        "\n",
        "Pictures"
      ],
      "metadata": {
        "id": "FqJCpsMRrzHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "\n",
        "metadata_df.describe(include='all')"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Metadata Dataset:**\n",
        "\n",
        "Each restaurant has a unique name and link.\n",
        "\n",
        "The 'Cost' variable shows a range of values, with 500 appearing most frequently.\n",
        "\n",
        "'Collections' and 'Cuisines' show a wide variety of categories and types.\n",
        "\n",
        "Most common timing is \"11 AM to 11 PM\"."
      ],
      "metadata": {
        "id": "0eA9jzyTr_pf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "\n",
        "reviews_df.describe(include='all')"
      ],
      "metadata": {
        "id": "b96nxgv-rcSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reviews Dataset:**\n",
        "\n",
        "100 unique restaurants are reviewed.\n",
        "\n",
        "The dataset features a large number of unique reviewers and reviews.\n",
        "\n",
        "The most common rating is 5.\n",
        "\n",
        "A significant portion of the reviews does not include pictures."
      ],
      "metadata": {
        "id": "GucfzCdKsGwg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Metadata Dataset:**\n",
        "\n",
        "**Name**: The name of the restaurant.\n",
        "\n",
        "**Links:** URL to the restaurant's Zomato page.\n",
        "\n",
        "**Cost:** Estimated cost for two people dining.\n",
        "\n",
        "**Collections:** Categories or collections the restaurant is part of.\n",
        "\n",
        "**Cuisines:** Types of cuisines offered by the rest"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reviews Dataset:**\n",
        "\n",
        "**Restaurant:** The name of the restaurant being reviewed.\n",
        "\n",
        "**Reviewer:** The name of the person who wrote the review.\n",
        "\n",
        "**Review:** The text of the review.\n",
        "\n",
        "**Rating:** Numerical rating given to the restaurant by the reviewer.\n",
        "\n",
        "**Metadata:** Additional information about the review, like the number of reviews and followers of the reviewer.\n",
        "\n",
        "**Time:** Date and time when the review was posted.\n",
        "\n",
        "**Pictures:** The number of pictures included in the review."
      ],
      "metadata": {
        "id": "hStl_o0ttX5y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Unique values in each column of the metadata dataset\n",
        "{col: metadata_df[col].nunique() for col in metadata_df.columns}"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Metadata Dataset:**\n",
        "\n",
        "Name: 105 unique restaurant names.\n",
        "\n",
        "Links: 105 unique links (each restaurant has a unique link).\n",
        "Cost: 29 unique cost values.\n",
        "\n",
        "Collections: 42 unique collections.\n",
        "\n",
        "Cuisines: 92 unique cuisines.\n",
        "\n",
        "Timings: 77 unique timings."
      ],
      "metadata": {
        "id": "GK7wYbIxuHB1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Unique values in each column of the reviews dataset\n",
        "{col: reviews_df[col].nunique() for col in reviews_df.columns}"
      ],
      "metadata": {
        "id": "ut5gvENBuAvd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reviews Dataset:**\n",
        "\n",
        "Restaurant: 100 unique restaurants reviewed.\n",
        "\n",
        "Reviewer: 7,446 unique reviewers.\n",
        "\n",
        "Review: 9,364 unique reviews.\n",
        "\n",
        "Rating: 10 unique rating scores.\n",
        "\n",
        "Metadata: 2,477 unique metadata entries.\n",
        "\n",
        "Time: 9,782 unique timestamps.\n",
        "\n",
        "Pictures: 36 unique counts of pictures included in reviews."
      ],
      "metadata": {
        "id": "Xz4ljrAmuTmt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Insights from Variables Analysis**\n",
        "\n",
        "The **metadata dataset** reveals a diverse range of restaurants with a variety of cuisines and collections. The variation in cost indicates a range of dining options from budget to high-end.\n",
        "\n",
        "The **reviews dataset** illustrates a broad spectrum of customer opinions and ratings. The high number of unique reviewers and reviews suggests a rich source of data for sentiment analysis.\n",
        "\n",
        "The variation in 'Timings' and 'Collections' in the metadata dataset, along with the diverse 'Ratings' and textual 'Reviews' in the reviews dataset, present ample opportunities for in-depth analysis, including trend identification and customer preference analysis."
      ],
      "metadata": {
        "id": "Ax2z9Nhrup4o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dropping duplicates\n",
        "\n",
        "# Metadata Dataset\n",
        "\n",
        "metadata_df = metadata_df.drop_duplicates()\n",
        "\n",
        "# Reviews Dataset\n",
        "\n",
        "reviews_df = reviews_df.drop_duplicates()\n",
        "\n"
      ],
      "metadata": {
        "id": "GPatOJjO6lZ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking null values after dropping duplicates\n",
        "\n",
        "metadata_df.isnull().sum()"
      ],
      "metadata": {
        "id": "R-C4OJTe7tEQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking null values after dropping duplicates\n",
        "\n",
        "reviews_df.isnull().sum()"
      ],
      "metadata": {
        "id": "Unr9oIxG7xBC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Handle Missing Values\n",
        "\n",
        "# Metadata Dataset\n",
        "\n",
        "# Replacing missing values in 'Collections' with 'Not Specified'\n",
        "metadata_df['Collections'] = metadata_df['Collections'].fillna('Not Specified')\n",
        "\n",
        "# Dropping the row with the missing value in the 'Timings' column\n",
        "metadata_df = metadata_df.dropna(subset=['Timings'])\n",
        "\n",
        "# Reviews Dataset\n",
        "\n",
        "# As the number of missing values in all columns is very less as compared to the size of the dataset, we can proceed dropping that values\n",
        "\n",
        "reviews_df.dropna(inplace = True)\n",
        "\n",
        "# Step 2: Data Transformation\n",
        "\n",
        "# Metadata dataset\n",
        "\n",
        "# Converting 'Cost' to numerical format in the metadata dataset\n",
        "metadata_df['Cost'] = metadata_df['Cost'].astype(str).str.replace(',', '').astype(int)\n"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking if we have successfully handled missing data in 'metadata_df'\n",
        "\n",
        "metadata_df.isnull().sum()"
      ],
      "metadata": {
        "id": "vKe38Bgw3Qif"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking if we have successfully handled missing data in 'reviews_df'\n",
        "\n",
        "reviews_df.isnull().sum()"
      ],
      "metadata": {
        "id": "FlfHCrO33jsX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Manipulations***\n",
        "\n",
        "**1. Handling Missing Values**\n",
        "\n",
        "\n",
        "\n",
        "*   **Metadata Dataset:** Missing values in the 'Collections' column were replaced with 'Not Specified'. This preserves the integrity of the dataset while acknowledging the lack of specific collection information.\n",
        "*   **Reviews Dataset:** Rows with missing 'Review' text were removed. Since the review text is essential for sentiment analysis, retaining rows without this information would not be meaningful.\n",
        "\n",
        "**2. Handling Duplicates**\n",
        "\n",
        "\n",
        "\n",
        "*   **Reviews Dataset:** Duplicate entries were removed to ensure the uniqueness of each review, which is crucial for accurate sentiment analysis.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**3. Data Transformation**\n",
        "\n",
        "\n",
        "*   **Metadata Dataset:** The 'Cost' column was transformed from a string to a numerical format. This involved removing commas from the cost values and converting them to integers. This change enables more straightforward numerical analyses, such as cost comparisons or aggregations."
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Insights***\n",
        "\n",
        "\n",
        "\n",
        "*   The 'Collections' column in the metadata dataset contains valuable categorical information for clustering or segmentation analysis, despite some data being unspecified.\n",
        "*   The cleaning of the reviews dataset, particularly the removal of duplicates and incomplete entries, ensures that the subsequent sentiment analysis and review-based insights will be based on complete and unique data points.\n",
        "\n",
        "\n",
        "*   The conversion of the 'Cost' column in the metadata dataset from text to numeric form opens up possibilities for quantitative analysis and correlations with other numerical variables, like ratings from reviews.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "i-YNKloayLOf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart 1 visualization code\n",
        "\n",
        "# Filtering out non-numeric values from the 'Rating' column\n",
        "numeric_ratings = pd.to_numeric(reviews_df['Rating'], errors='coerce')\n",
        "\n",
        "# Setting the aesthetic style of the plots\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "# Choosing a color palette\n",
        "bright_palette = sns.color_palette(\"mako\", 10)\n",
        "\n",
        "# Recreating the histogram with the filtered data using the brighter palette\n",
        "sns.histplot(numeric_ratings.dropna(), kde=True, bins=10, color=bright_palette[3])\n",
        "\n",
        "# Adding a line for mean\n",
        "plt.axvline(numeric_ratings.mean(), color='green', linestyle='dashed', linewidth=1)\n",
        "\n",
        "# Enhancing title and labels\n",
        "plt.title('Distribution of Ratings', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Rating', fontsize=12)\n",
        "plt.ylabel('Frequency', fontsize=12)\n",
        "\n",
        "# Displaying the plot\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A histogram is selected for its effectiveness in showing the distribution of a numerical variable. It allows us to see how the ratings are distributed and identify the most common ratings given by customers."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The histogram shows a concentration of ratings around the higher end, particularly around ratings of 4 and 5. This indicates that a significant number of reviews are positive."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, the insights can be positively impactful:\n",
        "\n",
        "**Positive Impact:** The skew towards higher ratings suggests general customer satisfaction, which is beneficial for the reputation of the restaurants on Zomato. It can also guide new customers in choosing well-rated restaurants.\n",
        "\n",
        "**Negative Growth Potential:** If not addressed, the smaller proportion of lower ratings could indicate specific areas where restaurants need to improve. Ignoring these could lead to negative customer experiences being overlooked."
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart 2 visualization code\n",
        "\n",
        "# Filtering top cuisines\n",
        "top_cuisines = metadata_df['Cuisines'].str.split(', ').explode().value_counts().head(10)\n",
        "\n",
        "# Setting the aesthetic style of the plots\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "# Choosing a color palette\n",
        "palette = sns.color_palette(\"viridis\", len(top_cuisines))\n",
        "\n",
        "# Plotting with enhanced aesthetics\n",
        "sns.barplot(x=top_cuisines.values, y=top_cuisines.index, palette=palette)\n",
        "\n",
        "# Adding titles and labels with improved font\n",
        "plt.title('Top 10 Cuisines Offered', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Number of Restaurants', fontsize=12)\n",
        "plt.ylabel('Cuisine', fontsize=12)\n",
        "\n",
        "# Improving layout\n",
        "plt.tight_layout()\n",
        "\n",
        "# Displaying the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A bar chart is effective for comparing categories. In this case, it helps in visualizing the most common cuisines offered across different restaurants.\n",
        "\n"
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This chart will show which cuisines are most prevalent among the restaurants listed on Zomato. Understanding popular cuisines can provide insights into customer preferences and market trends."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, this information is valuable for:\n",
        "\n",
        "**Positive Impact:** Restaurants and Zomato can tailor their offerings and marketing strategies based on popular cuisines. It aids in menu planning and promotional activities.\n",
        "\n",
        "**Negative Growth Potential:** Overemphasis on popular cuisines could lead to market saturation. Diversity in cuisines should be maintained to cater to a wide range of preferences."
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart 3 visualization code\n",
        "\n",
        "# Setting the aesthetic style of the plots\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "# Choosing a color for the boxplot\n",
        "color = sns.color_palette(\"hsv\", 1)\n",
        "\n",
        "# Creating the boxplot\n",
        "sns.boxplot(x=metadata_df['Cost'], color=color[0])\n",
        "\n",
        "# Adding titles and labels\n",
        "plt.title('Distribution of Average Cost for Two', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Cost', fontsize=12)\n",
        "\n",
        "# Improving layout\n",
        "plt.tight_layout()\n",
        "\n",
        "# Displaying the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A box plot is useful for depicting the distribution of a numerical variable. It shows the median, quartiles, and outliers, which helps in understanding the cost range."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart will reveal the typical cost range for dining at these restaurants, including the median cost and any outliers (extremely high or low costs)."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Positive Impact:** Understanding the cost distribution helps restaurants position themselves in the market (e.g., budget-friendly, premium) and helps customers make informed choices.\n",
        "\n",
        "**Negative Growth Potential:** If the costs are generally high, it might limit the customer base to only those willing or able to spend more."
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "\n",
        "# Filtering out 10 top reviewed restaurants\n",
        "top_reviewed_restaurants = reviews_df['Restaurant'].value_counts().head(10)\n",
        "\n",
        "# Setting the aesthetic style of the plots\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "# Choosing a color palette\n",
        "palette = sns.color_palette(\"cubehelix\", len(top_reviewed_restaurants))\n",
        "\n",
        "# Creating the barplot with the chosen palette\n",
        "sns.barplot(x=top_reviewed_restaurants.values, y=top_reviewed_restaurants.index, palette=palette)\n",
        "\n",
        "# Adding titles and labels with improved font\n",
        "plt.title('Top 10 Most Reviewed Restaurants', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Number of Reviews', fontsize=12)\n",
        "plt.ylabel('Restaurant', fontsize=12)\n",
        "\n",
        "# Improving layout\n",
        "plt.tight_layout()\n",
        "\n",
        "# Displaying the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A bar chart is effective for comparing the frequency or count of a categorical variable. In this case, it helps to identify which restaurants have the most reviews."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This chart will show us which restaurants are the most popular in terms of the number of reviews they receive, indicating high customer engagement."
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, the insights can be positively impactful:\n",
        "\n",
        "**Positive Impact:** Identifying the most reviewed restaurants can guide new customers and also help Zomato in highlighting popular spots.\n",
        "\n",
        "**Negative Growth Potential:** Restaurants with fewer reviews might need more visibility or improvements to attract more customers."
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart 5 visualization code\n",
        "\n",
        "# Setting the aesthetic style of the plots\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "# Choosing a color for the violin plot\n",
        "color = sns.color_palette(\"plasma\", 1)\n",
        "\n",
        "# Creating the violin plot\n",
        "sns.violinplot(x=metadata_df['Cost'], color=color[0])\n",
        "\n",
        "# Adding titles and labels\n",
        "plt.title('Cost Distribution of Restaurants', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Cost for Two', fontsize=12)\n",
        "\n",
        "# Improving layout\n",
        "plt.tight_layout()\n",
        "\n",
        "# Displaying the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A violin plot provides a deeper understanding of the distribution of a numerical variable, showing both the probability density and the range of the data."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This chart will help us understand the range and common cost brackets for dining out, as represented in the dataset."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Positive Impact:** Knowledge of common price ranges can help Zomato tailor its suggestions to users based on their budget preferences.\n",
        "\n",
        "**Negative Growth Potential:** If the majority of restaurants are clustered around a high price range, it might indicate a lack of affordable dining options on the platform."
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code\n",
        "\n",
        "# Preparing data for the chart\n",
        "reviews_df['Time'] = pd.to_datetime(reviews_df['Time'])\n",
        "review_trend = reviews_df.resample('M', on='Time')['Review'].count()\n",
        "\n",
        "# Plotting\n",
        "review_trend.plot(kind='line')\n",
        "plt.title('Review Count Trend Over Time')\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Number of Reviews')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A line chart is suitable for displaying trends over time. It helps to identify patterns or changes in the review activity."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart will reveal how the review counts have varied over time, indicating periods of high or low customer engagement."
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Positive Impact:** Identifying periods of high review activity can help in understanding customer engagement trends.\n",
        "\n",
        "**Negative Growth Potential:** Periods of low activity might indicate times when customer engagement dips, which could be important for marketing and promotional strategies."
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code\n",
        "\n",
        "# Define the top 10 most reviewed restaurants\n",
        "top_restaurants = reviews_df['Restaurant'].value_counts().head(10).index\n",
        "\n",
        "# Ensuring 'Rating' column is numeric\n",
        "reviews_df['Rating'] = pd.to_numeric(reviews_df['Rating'], errors='coerce')\n",
        "\n",
        "# Drop rows with null values in the 'Rating' column\n",
        "reviews_df.dropna(subset=['Rating'], inplace=True)\n",
        "\n",
        "# Calculating average ratings for the top 10 most reviewed restaurants\n",
        "average_ratings = reviews_df[reviews_df['Restaurant'].isin(top_restaurants)].groupby('Restaurant')['Rating'].mean()\n",
        "\n",
        "# Setting the aesthetic style of the plots\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "# Choosing a color palette\n",
        "palette = sns.color_palette(\"coolwarm\", len(average_ratings))\n",
        "\n",
        "# Creating the bar plot\n",
        "sns.barplot(x=average_ratings.values, y=average_ratings.index, palette=palette)\n",
        "\n",
        "# Adding titles and labels\n",
        "plt.title('Average Rating of Top 10 Most Reviewed Restaurants', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Average Rating', fontsize=12)\n",
        "plt.ylabel('Restaurant', fontsize=12)\n",
        "\n",
        "# Improving layout\n",
        "plt.tight_layout()\n",
        "\n",
        "# Displaying the plot\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bar chart is effective if we want the frequency of a continuous value and also want to compare categories"
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart shows varying average ratings among the most reviewed restaurants. Some highly reviewed restaurants maintain high average ratings, indicating consistent customer satisfaction.\n",
        "\n",
        "This insight is valuable for both customers and restaurant owners. High ratings in popular restaurants can attract more customers, while lower ratings might indicate areas needing improvement."
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Positive Business Impact:** Highlighting restaurants with high average ratings can enhance customer trust in the platform's recommendations.\n",
        "\n",
        "**Negative Growth Potential:** If popular restaurants have lower ratings, it might reflect on the platform's overall quality perception."
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code\n",
        "\n",
        "# Preparing data for the chart\n",
        "top_collections = metadata_df['Collections'].str.split(', ').explode().value_counts().head(10)\n",
        "\n",
        "# Dropping the first entry\n",
        "top_collections = top_collections.iloc[1:]\n",
        "\n",
        "# Setting the aesthetic style of the plots\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "# Choosing a vibrant color palette\n",
        "palette = sns.color_palette(\"tab10\", len(top_collections))\n",
        "\n",
        "# Creating the bar plot with the chosen palette\n",
        "sns.barplot(x=top_collections.values, y=top_collections.index, palette=palette)\n",
        "\n",
        "# Adding titles and labels with improved font\n",
        "plt.title('Top 10 Collections with Most Restaurants', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Number of Restaurants', fontsize=12)\n",
        "plt.ylabel('Collection', fontsize=12)\n",
        "\n",
        "# Improving layout\n",
        "plt.tight_layout()\n",
        "\n",
        "# Displaying the plot\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bar chart is effective if we want the frequency of a continuous value and also want to compare categories"
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart identifies the most prevalent collections among restaurants listed on Zomato. This reflects popular dining themes and trends."
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Positive Business Impact:** Zomato can use this information for targeted marketing, promoting popular collections to attract customers looking for specific dining experiences.\n",
        "\n",
        "**Business Strategy:** Understanding popular collections can also guide Zomato in suggesting restaurants join certain collections to increase visibility."
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 visualization code\n",
        "\n",
        "# Combining all review texts\n",
        "all_reviews = ' '.join(reviews_df['Review'].dropna())\n",
        "\n",
        "# Creating the word cloud with a lighter background\n",
        "wordcloud = WordCloud(\n",
        "    width = 800,\n",
        "    height = 800,\n",
        "    background_color ='white',  # using a white background\n",
        "    colormap='viridis',         # changing to a vibrant, yet light-friendly color scheme\n",
        "    max_words=150,              # limiting the number of words\n",
        "    contour_width=1,            # setting a subtle contour width\n",
        "    contour_color='steelblue'   # setting a suitable contour color\n",
        ").generate(all_reviews)\n",
        "\n",
        "# Displaying the word cloud\n",
        "plt.figure(figsize=(5, 6), facecolor=None)  # adjusting figure size\n",
        "plt.imshow(wordcloud, interpolation='bilinear')  # using bilinear interpolation for smoother appearance\n",
        "plt.axis(\"off\")\n",
        "plt.tight_layout(pad=0)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A word cloud is a visually appealing and intuitive way to showcase the most common words in a set of text data, such as customer reviews. It quickly highlights key themes or topics mentioned by customers."
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The word cloud reveals the most prominent words used in the reviews. Words that appear larger are mentioned more frequently, indicating common themes in customer feedback.\n",
        "\n"
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Positive Impact:** Frequent positive words can pinpoint strengths in customer experience, such as food quality, ambiance, or service, which can be leveraged in marketing and promotion.\n",
        "\n",
        "**Negative Growth Potential:** If negative terms are prominent, it could signify common issues that need to be addressed to improve customer satisfaction."
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 10 visualization code\n",
        "\n",
        "# Preparing data for the chart\n",
        "# Extracting the primary cuisine for each restaurant\n",
        "metadata_df['Primary_Cuisine'] = metadata_df['Cuisines'].apply(lambda x: x.split(', ')[0] if pd.notnull(x) else 'Unknown')\n",
        "\n",
        "# Creating cost bins\n",
        "metadata_df['Cost_Bin'] = pd.cut(metadata_df['Cost'], bins=[0, 500, 1000, 1500, 2000, 2500, 3000, 3500, 4000, 4500, 5000],\n",
        "                                 labels=['0-500', '500-1000', '1000-1500', '1500-2000', '2000-2500', '2500-3000', '3000-3500', '3500-4000', '4000-4500', '4500-5000'])\n",
        "\n",
        "# Merging with reviews dataset to get review counts\n",
        "combined_df = pd.merge(metadata_df, reviews_df, left_on='Name', right_on='Restaurant')\n",
        "review_count_heatmap_data = combined_df.groupby(['Primary_Cuisine', 'Cost_Bin']).size().unstack(fill_value=0)\n",
        "\n",
        "# Setting the aesthetic style of the plots\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "# Plotting with enhanced aesthetics\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.heatmap(review_count_heatmap_data, annot=True, cmap='cividis', fmt='g')\n",
        "\n",
        "# Adding titles and labels with improved font\n",
        "plt.title('Review Counts by Cuisine Type and Cost Range', fontsize=16, fontweight='bold')\n",
        "plt.xlabel('Cost Range', fontsize=14)\n",
        "plt.ylabel('Cuisine Type', fontsize=14)\n",
        "\n",
        "# Improving layout\n",
        "plt.tight_layout()\n",
        "\n",
        "# Displaying the plot\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A heatmap is an effective tool for visualizing complex data in a two-dimensional representation. It is particularly useful for showing the relationship between multiple variables, in this case, cuisine type, cost range, and review counts."
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart reveals popular combinations of cuisine and cost, as indicated by the density of reviews. For instance, certain cuisine types might be more popular in specific cost ranges, and this popularity is reflected in the number of reviews.\n",
        "\n"
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "JMzcOPDDphqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Positive Impact:** This analysis can guide restaurants and Zomato in understanding consumer preferences, helping them tailor their offerings and marketing strategies. For example, focusing on popular cuisine-cost combinations could attract more customers.\n",
        "\n",
        "**Negative Growth Potential:** Less reviewed cuisine-cost combinations might indicate neglected market segments or areas needing improvement. Identifying these gaps provides an opportunity for growth by catering to underserved markets or improving offerings."
      ],
      "metadata": {
        "id": "R4Ka1PC2phqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 11 visualization code\n",
        "\n",
        "# Preparing data for the chart\n",
        "cuisine_distribution = metadata_df['Primary_Cuisine'].value_counts().head(10)\n",
        "\n",
        "# Choosing a color palette\n",
        "colors = plt.cm.tab20c.colors  # using a matplotlib colormap\n",
        "\n",
        "# Plotting with enhanced aesthetics\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.pie(cuisine_distribution, labels=cuisine_distribution.index, autopct='%1.1f%%', startangle=140, colors=colors)\n",
        "plt.title('Restaurant Distribution by Primary Cuisine', fontsize=14, fontweight='bold')\n",
        "\n",
        "# Displaying the plot\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A pie chart is chosen for its ability to visually represent the proportion of categories in a dataset. It's particularly useful for understanding the market share of different cuisines among the restaurants."
      ],
      "metadata": {
        "id": "-vsMzt_np1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "8zGJKyg5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart highlights the most prevalent cuisines among the restaurants. It shows which cuisines are more common, indicating potential customer preferences or market trends."
      ],
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "PVzmfK_Ep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Positive Impact:** Identifying the most popular cuisines can help Zomato and restaurant owners understand customer preferences, enabling them to cater to popular tastes or offer promotions on trending cuisines.\n",
        "\n",
        "**Negative Growth Potential:** A concentration in certain cuisines might point to a lack of diversity in the culinary landscape. This could be an opportunity to introduce or promote underrepresented cuisines, appealing to a broader customer base."
      ],
      "metadata": {
        "id": "druuKYZpp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 12"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 12 visualization code\n",
        "\n",
        "# Resampling data to monthly frequency and calculating average rating\n",
        "monthly_avg_rating = reviews_df.resample('M', on='Time')['Rating'].mean()\n",
        "\n",
        "# Setting the aesthetic style of the plots\n",
        "plt.style.use('seaborn-darkgrid')\n",
        "\n",
        "# Visualization\n",
        "plt.figure(figsize=(12, 6))\n",
        "monthly_avg_rating.plot(color='dodgerblue', linewidth=2, marker='o', markersize=6)\n",
        "\n",
        "# Adding titles and labels with improved font\n",
        "plt.title('Monthly Average Ratings Over Time', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Time', fontsize=12)\n",
        "plt.ylabel('Average Rating', fontsize=12)\n",
        "\n",
        "# Improving layout\n",
        "plt.tight_layout()\n",
        "\n",
        "# Adding a grid for better readability\n",
        "plt.grid(True)\n",
        "\n",
        "# Displaying the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose a time-series line chart because it is excellent for visualizing how a particular metric (average rating) changes over time. This type of chart helps in identifying trends, seasonal patterns, or any anomalies in the data across different time intervals.\n",
        "\n"
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OwCH7GScSb7j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart is expected to reveal how customer ratings have fluctuated over time. Key insights could include identifying periods of high or low ratings, noting any seasonal trends in customer satisfaction, and observing the general trajectory of ratings  whether they are improving, declining, or remaining stable."
      ],
      "metadata": {
        "id": "IQMY2xUkSb7j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "KtJbMZtuSb7k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Positive Impact:** Understanding the trend of customer ratings over time can be crucial for strategic planning. For instance, identifying times of year when ratings dip can prompt targeted improvements or promotions. Likewise, understanding when ratings peak can help capitalize on and reinforce what is working well.\n",
        "\n",
        "**Negative Growth Potential:** A downward trend in ratings could signify a decline in customer satisfaction, signaling a need for immediate attention and corrective action. Identifying these trends early can help prevent long-term negative impacts on the business."
      ],
      "metadata": {
        "id": "eibJ9lygSb7k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 13"
      ],
      "metadata": {
        "id": "Ag9LCva-p1cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 13 visualization code\n",
        "\n",
        "# Grouping by primary cuisine and calculating average rating\n",
        "cuisine_rating_data = combined_df.groupby('Primary_Cuisine')['Rating'].mean().sort_values(ascending=False).head(10)\n",
        "\n",
        "# Setting the aesthetic style of the plots\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "# Visualization\n",
        "cuisine_rating_data.plot(kind='bar', color='teal', figsize=(10, 6))\n",
        "\n",
        "# Adding titles and labels with improved font\n",
        "plt.title('Average Rating by Cuisine Type', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Cuisine Type', fontsize=12)\n",
        "plt.ylabel('Average Rating', fontsize=12)\n",
        "\n",
        "# Improving layout\n",
        "plt.tight_layout()\n",
        "\n",
        "# Displaying the plot\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "EUfxeq9-p1cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "E6MkPsBcp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A stacked bar chart is effective for comparing the distribution of a numerical variable (average rating) across different categories (cuisine types). It provides a clear visual representation of how ratings vary across cuisines.\n",
        "\n"
      ],
      "metadata": {
        "id": "V22bRsFWp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "2cELzS2fp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This chart will show which cuisines receive the highest average ratings, indicating customer satisfaction and popularity."
      ],
      "metadata": {
        "id": "ozQPc2_Ip1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "3MPXvC8up1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Positive Impact:** Understanding which cuisines receive higher ratings can guide restaurant owners and Zomato in promoting these popular cuisines, potentially driving more business.\n",
        "\n",
        "**Negative Growth Potential:** Cuisines with lower average ratings may indicate areas where improvements are needed or where customer expectations are not being met."
      ],
      "metadata": {
        "id": "GL8l1tdLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 14 - Correlation Heatmap (Key Numerical Features)"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 14 visualization code\n",
        "\n",
        "# Preparing data for the chart\n",
        "# Merging metadata and reviews datasets\n",
        "combined_data = pd.merge(metadata_df, reviews_df, how='left', left_on='Name', right_on='Restaurant')\n",
        "\n",
        "# Selecting numeric features for correlation analysis\n",
        "numeric_features = combined_data[['Cost', 'Rating', 'Pictures']]\n",
        "correlation_matrix = numeric_features.corr()\n",
        "\n",
        "# Setting the aesthetic style of the plots\n",
        "sns.set_style(\"white\")\n",
        "\n",
        "# Plotting with enhanced aesthetics\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='Blues', fmt='.2f', annot_kws={'size': 12})\n",
        "\n",
        "# Adding title with improved font\n",
        "plt.title('Correlation Heatmap Between Numeric Features', fontsize=14, fontweight='bold')\n",
        "\n",
        "# Improving layout\n",
        "plt.tight_layout()\n",
        "\n",
        "# Displaying the plot\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ylSl6qgtp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A correlation heatmap is a powerful tool for visualizing the relationships between multiple numeric variables. It helps in identifying potential correlations or lack thereof, which can be insightful for understanding how different aspects of the data relate to each other.\n",
        "\n"
      ],
      "metadata": {
        "id": "m2xqNkiQp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ZWILFDl5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The heatmap will show the strength and direction (positive or negative) of the relationships between features like cost, rating, and the number of pictures. For example, it might reveal if higher-cost restaurants tend to have higher ratings."
      ],
      "metadata": {
        "id": "x-lUsV2mp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "M7G43BXep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Positive Impact:** Understanding these correlations can help businesses make data-driven decisions. For instance, if there's a strong correlation between cost and ratings, restaurants might consider their pricing strategies to improve customer satisfaction.\n",
        "\n",
        "**Negative Growth Potential:** Identifying negative correlations or lack of expected correlations can highlight areas that may require further investigation or strategy adjustments."
      ],
      "metadata": {
        "id": "5wwDJXsLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 15 - Pair Plot (Key Features)"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 15 visualization code\n",
        "\n",
        "# Selecting key variables for the pair plot\n",
        "selected_columns = ['Cost', 'Rating', 'Pictures']\n",
        "pair_plot_data = combined_data[selected_columns]\n",
        "\n",
        "# Creating the pair plot\n",
        "sns.pairplot(pair_plot_data)\n",
        "plt.suptitle('Pair Plot of Cost, Rating, and Pictures', y=1.02)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We picked this chart so as to analyse teh relationships between key features"
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that there is not much correelations between the key variables or any visible pattern."
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 16"
      ],
      "metadata": {
        "id": "04Uh5c6cSVyd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 16 visualization code\n",
        "\n",
        "# Grouping data by primary cuisine and rating\n",
        "cuisine_rating_counts = combined_df.groupby(['Primary_Cuisine', 'Rating']).size().unstack().fillna(0)\n",
        "\n",
        "# Creating a stacked bar chart\n",
        "cuisine_rating_counts.plot(kind='bar', stacked=True, figsize=(12, 6))\n",
        "plt.title('Restaurant Ratings by Cuisine Type')\n",
        "plt.xlabel('Cuisine Type')\n",
        "plt.ylabel('Number of Ratings')\n",
        "plt.legend(title='Rating', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "5CSmvXUgWuS1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "AqfxZshLXQ9M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The stacked bar chart is an effective way to visualize the distribution of ratings across different cuisine types in a compact form. Each segment's size within a bar reflects the count of ratings for that score, allowing for a comparison both within and across the cuisine types."
      ],
      "metadata": {
        "id": "r4Ivh9qjXQ9M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "qrwQK7_SXQ9N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart reveals how different cuisine types fare in terms of ratings. It shows not only the total number of ratings each cuisine has received but also how those ratings are distributed across the rating scale. For instance, we can see which cuisines have a higher proportion of excellent ratings (5.0) and which ones have more lower-end ratings."
      ],
      "metadata": {
        "id": "nvWMVMRWXQ9N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "LKYAbypkXQ9N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Positive Impact:** This visualization can help restaurant owners and Zomato understand which cuisines are most popular and well-received by customers, guiding them on what to feature or promote. For example, if certain cuisines consistently receive high ratings, they might warrant additional marketing focus or expansion in the menu offerings.\n",
        "\n",
        "**Negative Growth Potential:** On the other hand, cuisines with a significant proportion of lower ratings might require attention. These insights can inform decisions to improve the quality, presentation, or perhaps the variety of dishes within those cuisine types to enhance customer satisfaction and ratings."
      ],
      "metadata": {
        "id": "0biIgmRTXQ9N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Null Hypothesis (H0)**: There is no significant difference in the average ratings between high-cost and low-cost restaurants.\n",
        "\n",
        "**Alternate Hypothesis (H1)**: High-cost restaurants have a significantly different average rating compared to low-cost restaurants."
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "\n",
        "# Calculating the average rating and cost for each restaurant\n",
        "avg_rating_cost = combined_df.groupby('Restaurant').agg({'Rating': 'mean', 'Cost': 'mean'}).dropna()\n",
        "\n",
        "# Defining high-cost and low-cost restaurants based on the median cost\n",
        "median_cost = avg_rating_cost['Cost'].median()\n",
        "high_cost_ratings = avg_rating_cost[avg_rating_cost['Cost'] > median_cost]['Rating']\n",
        "low_cost_ratings = avg_rating_cost[avg_rating_cost['Cost'] <= median_cost]['Rating']\n",
        "\n",
        "# Performing a two-sample t-test (Welch's t-test)\n",
        "t_stat, p_value = stats.ttest_ind(high_cost_ratings, low_cost_ratings, equal_var=False)\n",
        "\n",
        "# Printing the test statistic and p-value\n",
        "print(f\"T-Statistic: {t_stat}, P-Value: {p_value}\")\n"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Two-Sample T-Test (Welch's T-Test)"
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The reasons for choosing this test are as follows:**\n",
        "\n",
        "Comparing Means of Two Independent Groups: The goal was to compare the means of two independent groups - high-cost and low-cost restaurants. Welch's t-test is specifically designed to compare the means of two groups to see if they are significantly different from each other.\n",
        "\n",
        "Does Not Assume Equal Variances: Unlike the standard t-test, Welch's t-test does not assume that the two populations have equal variances. This is particularly important when dealing with real-world data, where this assumption can often be violated. Given that high-cost and low-cost restaurants might vary significantly in their operational dynamics and customer perceptions, it's safer not to assume equal variances.\n",
        "\n",
        "Robust to Sample Size Differences: Welch's t-test is more robust than the standard t-test when the two samples have unequal sizes. This is useful in cases where the distribution of restaurants across the high-cost and low-cost categories is uneven.\n",
        "\n",
        "Appropriateness for the Data: The data was assumed to be approximately normally distributed for each group, which is a prerequisite for applying t-tests. Given that we're dealing with average ratings, this assumption is generally reasonable for a large enough sample size.\n",
        "\n",
        "In summary, Welch's t-test was chosen due to its suitability for comparing the means of two independent samples without assuming equal variances and its robustness in handling potential discrepancies in sample sizes and variances between the two groups of restaurants."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interpretation:\n",
        "\n",
        "The **T-Statistic** is a measure of the difference between the two groups relative to the variation in the sample data. A higher absolute value of the T-Statistic indicates a greater difference between the groups.\n",
        "\n",
        "The **P-Value** is very small (significantly less than 0.05). This indicates that the probability of observing such a substantial difference in average ratings between high-cost and low-cost restaurants, assuming the null hypothesis is true (i.e., there is no difference), is extremely low."
      ],
      "metadata": {
        "id": "Bqte0KgTG7TI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusion:**\n",
        "\n",
        "Since the p-value is significantly less than the typical alpha level of 0.05, we reject the null hypothesis. This suggests that there is a statistically significant difference in the average ratings between high-cost and low-cost restaurants.\n",
        "\n",
        "In the context of your hypothetical statement, this result supports the idea that high-cost restaurants have a significantly different average rating compared to low-cost restaurants."
      ],
      "metadata": {
        "id": "3fOvHHRaHLAg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Null Hypothesis (H0):** There is no correlation between the number of reviews a restaurant receives and its average rating.\n",
        "\n",
        "**Alternate Hypothesis (H1):** There is a positive correlation between the number of reviews a restaurant receives and its average rating."
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert 'Review' and 'Rating' columns to numeric, handling non-numeric values\n",
        "combined_df['Review'] = pd.to_numeric(combined_df['Review'], errors='coerce')\n",
        "combined_df['Rating'] = pd.to_numeric(combined_df['Rating'], errors='coerce')\n",
        "\n",
        "# Drop rows with NaN values in these columns\n",
        "combined_df = combined_df.dropna(subset=['Review', 'Rating'])\n",
        "\n",
        "# Calculate Pearson's correlation coefficient\n",
        "correlation_coef, p_value = stats.pearsonr(combined_df['Review'], combined_df['Rating'])\n",
        "\n",
        "# Print the correlation coefficient and p-value\n",
        "print(f\"Pearson Correlation Coefficient: {correlation_coef}, P-Value: {p_value}\")\n"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pearson's Correlation Coefficient"
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Nature of the Variables:** Pearson's correlation coefficient is appropriate for measuring the strength and direction of the linear relationship between two continuous variables. In this case, we are examining the relationship between two continuous variables: the number of reviews (which is a count) and the average rating (a continuous metric).\n",
        "\n",
        "**Quantitative Analysis:** Pearson's correlation provides a quantitative measure (the correlation coefficient) that ranges from -1 to +1, where +1 indicates a perfect positive linear relationship, -1 indicates a perfect negative linear relationship, and 0 indicates no linear relationship. This quantitative aspect allows for a precise assessment of the relationship.\n",
        "\n",
        "**Assumption of Linearity:** The test assumes that the relationship between the variables is linear, meaning that as one variable increases or decreases, the other variable also increases or decreases in a proportional and constant rate. This assumption was deemed reasonable for the relationship between the number of reviews and ratings.\n",
        "\n",
        "**Popularity and Reliability:** Pearson's correlation coefficient is widely used in statistical analysis and is a well-established method for investigating linear relationships, making it a reliable choice for this kind of analysis.\n",
        "\n",
        "**Ease of Interpretation:** The results of Pearson's correlation are straightforward to interpret, making it a practical choice for hypothesis testing where the goal is to clearly understand and communicate the nature of the relationship between variables.\n",
        "\n",
        "In summary, Pearson's correlation coefficient was chosen due to its suitability for assessing linear relationships between two continuous variables, its ease of interpretation, and its reliability as a standard method in statistical analysis."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interpretation:\n",
        "\n",
        "The **Pearson Correlation Coefficient** is approximately 0.96, which is very close to 1. This indicates a very strong positive linear relationship between the number of reviews a restaurant receives and its average rating.\n",
        "\n",
        "The **P-Value is about 0.042**, which is less than the typical alpha level of 0.05. This suggests that the observed correlation is statistically significant and not just due to random chance."
      ],
      "metadata": {
        "id": "Ins8CPDDyDe8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusion:**\n",
        "\n",
        "Given the high correlation coefficient and the low p-value, we can reject the null hypothesis (H0) and accept the alternate hypothesis (H1). This means that, according to your dataset, there is a statistically significant positive correlation between the number of reviews a restaurant receives and its average rating. In other words, restaurants with more reviews tend to have higher ratings.\n",
        "\n",
        "This result aligns with the common intuition that more frequently reviewed restaurants, which often indicate popularity or high customer traffic, are likely to have higher ratings.\n",
        "\n",
        "With this conclusion for Hypothetical Statement 2, we can now proceed to Hypothetical Statement 3. Please let me know when you are ready to move on or if there are any other aspects of the project you would like to address!\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pvPfjmQyyHR3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Null Hypothesis (H0):** There is no significant difference in average ratings between restaurants offering a high number of cuisines and those offering a low number of cuisines.\n",
        "\n",
        "**Alternate Hypothesis (H1):**  Restaurants offering a high number of cuisines have a significantly different average rating compared to those offering a low number of cuisines."
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating 'NumCuisines' by counting the number of cuisines for each restaurant\n",
        "combined_df['NumCuisines'] = combined_df['Cuisines'].str.split(', ').apply(lambda x: len(x) if x is not None else 0)\n",
        "\n",
        "# Define high-cuisine and low-cuisine groups based on a threshold (e.g., median)\n",
        "cuisine_threshold = combined_df['NumCuisines'].median()\n",
        "high_cuisine_group = combined_df[combined_df['NumCuisines'] > cuisine_threshold]['Rating']\n",
        "low_cuisine_group = combined_df[combined_df['NumCuisines'] <= cuisine_threshold]['Rating']\n",
        "\n",
        "# Perform Welch's T-test\n",
        "t_stat, p_value = stats.ttest_ind(high_cuisine_group, low_cuisine_group, equal_var=False)\n",
        "\n",
        "# Print the T-Statistic and P-Value\n",
        "print(f\"T-Statistic: {t_stat}, P-Value: {p_value}\")\n"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Two-Sample T-Test (Welch's T-Test)"
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Two-Sample T-Test (specifically, Welch's T-Test) was chosen for this analysis because:\n",
        "\n",
        "*   **Comparison of Two Independent Groups**: The goal was to compare the average ratings between two distinct groups of restaurants - those offering a high number of cuisines and those with a lower number.\n",
        "*   **No Assumption of Equal Variances**: Welch's T-Test does not assume equal variances between the groups, making it suitable for real-world data where this assumption may not hold.\n",
        "*   **Appropriateness for the Data**: This test is appropriate for comparing the means of two groups when we do not have paired data and the groups are independent of each other."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Interpretation\n",
        "\n",
        "*   **T-Statistic**: The T-Statistic value is 0.24253562503633297, indicating a minimal difference between the two groups.\n",
        "*   **P-Value**: The P-Value is 0.8450283873936709, significantly higher than the 0.05 threshold, suggesting that the observed difference in ratings could be due to random chance."
      ],
      "metadata": {
        "id": "0oKs9o-V0L3w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusion**:\n",
        "\n",
        "*   Given the high P-Value, we fail to reject the null hypothesis (H0). This indicates that, according to the dataset, there is no statistically significant difference in average ratings between restaurants offering a high number of cuisines and those offering fewer.\n",
        "*   The conclusion is that the variety of cuisines offered by a restaurant does not significantly impact its average rating. Other factors may be more influential in determining a restaurant's rating."
      ],
      "metadata": {
        "id": "CtiKYwSX0PZa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing (For clustering - Zomato Restaurant Clustering)***\n",
        "\n",
        "(We will do sentiment analysis(the second part of the project) after completing clustering)"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "\n",
        "# We already handled missing value in Data Wrangling in start above"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking for outliers in the numerical columns\n",
        "# For this dataset, the 'Cost' column is the primary numerical column\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "# Create a subplot with 1 row and 2 columns\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.boxplot(metadata_df['Cost'])\n",
        "plt.title('Boxplot of Cost')\n",
        "plt.xlabel('Cost')\n",
        "\n",
        "# Create a histogram of the 'Cost' column\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.histplot(metadata_df['Cost'], bins=20, kde=True)\n",
        "plt.title('Distribution of Cost')\n",
        "plt.xlabel('Cost')\n",
        "\n",
        "plt.tight_layout()  # Ensure proper spacing between subplots\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Identifying outliers in the 'Cost' column\n",
        "\n",
        "# Considering values beyond 1.5 times the IQR (Interquartile Range) as outliers\n",
        "\n",
        "Q1 = metadata_df['Cost'].quantile(0.25)\n",
        "Q3 = metadata_df['Cost'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "lower_bound = Q1 - 1.5 * IQR\n",
        "upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "# Filtering out the outliers\n",
        "data_no_outliers = metadata_df[(metadata_df['Cost'] >= lower_bound) & (metadata_df['Cost'] <= upper_bound)]\n",
        "\n",
        "# Checking the shape of the dataset before and after outlier removal\n",
        "original_shape = metadata_df.shape\n",
        "new_shape = data_no_outliers.shape\n",
        "\n",
        "# Reassigning the dataframe after outlier removal\n",
        "metadata_df = data_no_outliers\n",
        "\n",
        "original_shape, new_shape\n"
      ],
      "metadata": {
        "id": "he-joFCfz9V6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Originally, the dataset contained 104 entries. After removing outliers, it now contains 102 entries.\n",
        "\n"
      ],
      "metadata": {
        "id": "cMxD4IwK0W67"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The boxplot of the 'Cost' column shows some outliers, particularly on the higher end. We have a few options for handling these outliers:\n",
        "\n",
        "Trimming: Remove the outlier observations.\n",
        "\n",
        "Capping: Cap the values at a certain upper limit based on a percentile.\n",
        "\n",
        "Transformation: Apply a transformation to reduce the skewness.\n",
        "\n",
        "---\n",
        "\n",
        "As there are only two outliers. We will proceed removing them without significantly affecting the amount of data we have\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "As the distribution shows, the data is right skewed, so we can go with IQR method and trim the outliers.\n",
        "\n"
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Peeking over df\n",
        "metadata_df"
      ],
      "metadata": {
        "id": "RDeaGA80Apxd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Encoding Categorical Variables:** We'll use one-hot encoding for categorical variables. This method is suitable for our dataset since it doesn't imply an ordinal relationship between categories.\n",
        "\n",
        "**Feature Scaling:** We'll apply standard scaling (z-score normalization) to the numerical features to standardize their range."
      ],
      "metadata": {
        "id": "nAZvnDldAJFB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Selecting categorical columns for encoding\n",
        "categorical_cols = ['Primary_Cuisine']\n",
        "\n",
        "# Applying one-hot encoding\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "encoded_data = encoder.fit_transform(metadata_df[categorical_cols])\n",
        "\n",
        "# Converting the encoded data into a DataFrame\n",
        "encoded_df = pd.DataFrame(encoded_data, columns=encoder.get_feature_names_out(categorical_cols))\n",
        "\n",
        "metadata_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "encoded_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Combine the encoded features with the rest of the 'metadata_df'\n",
        "metadata_df = pd.concat([metadata_df, encoded_df], axis=1)\n",
        "\n",
        "# Dropping the original categorical columns as they are now encoded\n",
        "metadata_df.drop(categorical_cols, axis=1, inplace=True)\n",
        "\n",
        "metadata_df.head()  # Displaying the first few rows of the updated 'metadata_df' with encoded columns\n",
        "\n",
        "from sklearn.preprocessing import OrdinalEncoder\n"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ordinal encoding the 'Cost_Bin' column as it was numerical data but was in ordinal category.\n",
        "\n",
        "# Initialize OrdinalEncoder\n",
        "encoder = OrdinalEncoder()\n",
        "\n",
        "# Reshape the data since the encoder expects 2D input\n",
        "cost_bin_reshaped = metadata_df['Cost_Bin'].values.reshape(-1, 1)\n",
        "\n",
        "# Fit and transform the data\n",
        "metadata_df['Cost_Bin'] = encoder.fit_transform(cost_bin_reshaped)"
      ],
      "metadata": {
        "id": "1JfzfG10qofj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace NaN values with empty strings in 'Collections' and 'Cuisines' columns\n",
        "metadata_df['Collections'].fillna('', inplace=True)\n",
        "metadata_df['Cuisines'].fillna('', inplace=True)\n",
        "\n",
        "# Applying TF-IDF Vectorization to 'Collections' and 'Cuisines'\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=50)  # Limiting to top 50 features for simplicity\n",
        "\n",
        "# 'Collections' column\n",
        "collections_tfidf = tfidf_vectorizer.fit_transform(metadata_df['Collections'])\n",
        "collections_df = pd.DataFrame(collections_tfidf.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
        "\n",
        "# 'Cuisines' column\n",
        "cuisines_tfidf = tfidf_vectorizer.fit_transform(metadata_df['Cuisines'])\n",
        "cuisines_df = pd.DataFrame(cuisines_tfidf.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
        "\n",
        "\n",
        "# Combining the TF-IDF features with the main dataset\n",
        "metadata_df = pd.concat([metadata_df, collections_df, cuisines_df], axis=1)\n",
        "\n",
        "# Removing the original 'Collections' and 'Cuisines' columns as they are now represented numerically\n",
        "metadata_df.drop(['Collections', 'Cuisines'], axis=1, inplace=True)\n",
        "\n",
        "metadata_df.head()  # Displaying the first few rows of the updated dataset\n"
      ],
      "metadata": {
        "id": "Nh7oUExLEZvv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We first applied one-hot-encoding on important categorical feature 'Primary_Cuisine' that would not create too many dimensions.\n",
        "\n",
        "Then we ordinal encoded the 'Cost_Bin' column as that were having categories that was numerical and ordinal in nature\n",
        "\n",
        "Then there were other categorical data that were having too many unqiue values that we couldn't one-hot-encode as it would have then made too many dimensions and then our model would have suffered with curse of dimensionality.\n",
        "\n",
        "For the features were not scaled or transformed (like 'Collections', 'Cuisines', and 'Timings'), we had several options to make them suitable for clustering algorithms. These options primarily involved converting textual data into a numerical format:\n",
        "\n",
        "We used TF-IDF for 'Collections' and 'Cuisines', and basic feature engineering for 'Timings'."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select your features wisely to avoid overfitting\n",
        "\n",
        "# Dropping 'Timings' column as it is ambiguous and has many incorrect formats. Also, it is not important for further clustering\n",
        "\n",
        "metadata_df.drop(columns = ['Timings', 'Name', 'Links'], inplace = True)"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We selected important features that were relevant to clustering such as Cost, Primary_Cuisine and then we applied TF-IDF vectorization to bring highly categorical variables to numerical form to over curse of dimensionality"
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As the above mentioned features are most important to the business domain of restaurant clustering and logically can contribute the most to the clustering factor"
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data\n",
        "\n",
        "# Already Transformed data in steps above"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We just have to scale 'Cost' column.\n",
        "# Let's check the distribution of it to decide how to scale its values\n",
        "\n",
        "# Plot the distribution of 'Cost' before scaling\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.histplot(metadata_df['Cost'], bins=30, kde=True)  # Adjust the number of bins as needed\n",
        "plt.title('Distribution of Cost (Before Scaling)')\n",
        "plt.xlabel('Cost')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "RyEvQTQyt-gB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As the distribution of 'Cost' column is right skewed, we would applu **Log Transformation** to it"
      ],
      "metadata": {
        "id": "7DuVxiJk3oiZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply a logarithmic transformation to 'Cost'\n",
        "metadata_df['Cost'] = np.log1p(metadata_df['Cost'])\n",
        "\n",
        "# Display the first few rows of the dataset after the transformation\n",
        "metadata_df.head()\n"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have transformed (scaled) the 'Cost' column. Let's check that what si tha max value of 'Cost' column to check its scale"
      ],
      "metadata": {
        "id": "WpTjllz_4Er2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking maximum value of cost to see that it has successfully transformed to normal scale\n",
        "metadata_df['Cost'].max()"
      ],
      "metadata": {
        "id": "soGS9UhY37Rf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can go with this transformation.\n",
        "\n",
        "'Cost' is tranformed to normal scaled successfully"
      ],
      "metadata": {
        "id": "SPn8Nr0C4hrv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As the distribution is right skewed we have applied log transformation and scaled the data."
      ],
      "metadata": {
        "id": "mviqVEm7UYLk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We were having 2 very highly textual columns in the dataset, on which if we apply one hot encoding it would have generated too many columns. So we did TF-IDF vectorization on it and that too created many columns.\n",
        "\n",
        "Now we can apply PCA on this data to reduce its dimensionality and then we can cluster the data on reduced dimensions, but then we will loose the meaning of the data and that will not help us in deriving conclusions of cluster characteristics and behaviour. So we will not proceed with dimensionality reduction for now so as to derive characteristics of the cluster. However applying PCA is recommneded for highly dimensinal data.\n",
        "\n",
        "However, we will use PCA further to visualize our data and clusters into lower dimensions"
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As it is unsupervised problem, data splitting is not required"
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As it is unsupervised problem, we donot have a target feature and if we do not have target feature there is no point of imbalanced dataset"
      ],
      "metadata": {
        "id": "qRmUs5M-bTPh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation (For clustering - Zomato Restaurant Clustering)***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our approach for model implementation for this clustering task will be:\n",
        "\n",
        "1.  **K-Means Clustering**:\n",
        "    \n",
        "    *   Determine the optimal number of clusters using both the silhouette score and the elbow method.\n",
        "    *   Train the K-Means model and plot these metrics for better visualization and decision-making.\n",
        "2.  **Hierarchical Clustering**:\n",
        "    \n",
        "    *   Perform hierarchical clustering and compare the suggested number of clusters with that from K-Means.\n",
        "    *   Use dendrogram for visual analysis.\n",
        "3.  **DBSCAN**:\n",
        "    \n",
        "    *   Apply DBSCAN clustering.\n",
        "    *   Analyze its results, particularly how it handles noise and cluster formation."
      ],
      "metadata": {
        "id": "MffpUG2PQ_dI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1 - **K-Means**"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "\n",
        "# Preparing the data for clustering (excluding non-numeric columns)\n",
        "data_clustering = metadata_df.select_dtypes(include=[np.number])\n",
        "\n",
        "# Range of possible clusters to evaluate\n",
        "cluster_range = range(2, 11)\n",
        "\n",
        "# Variables to store results\n",
        "silhouette_scores = []\n",
        "inertia_values = []\n",
        "\n",
        "# Calculating silhouette scores and inertia (within-cluster sum of squares) for different cluster numbers\n",
        "for k in cluster_range:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    # Fit the Algorithm\n",
        "    kmeans.fit(data_clustering)\n",
        "    cluster_labels = kmeans.labels_\n",
        "\n",
        "    # Silhouette score\n",
        "    silhouette_avg = silhouette_score(data_clustering, cluster_labels)\n",
        "    silhouette_scores.append(silhouette_avg)\n",
        "\n",
        "    # Inertia\n",
        "    inertia_values.append(kmeans.inertia_)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the results\n",
        "plt.figure(figsize=(18, 8))\n",
        "\n",
        "# Silhouette score plot\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(cluster_range, silhouette_scores, marker='o')\n",
        "plt.title(\"Silhouette Scores for Different Numbers of Clusters\")\n",
        "plt.xlabel(\"Number of Clusters\")\n",
        "plt.ylabel(\"Silhouette Score\")\n",
        "\n",
        "# Elbow method plot\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(cluster_range, inertia_values, marker='o')\n",
        "plt.title(\"Elbow Method for Different Numbers of Clusters\")\n",
        "plt.xlabel(\"Number of Clusters\")\n",
        "plt.ylabel(\"Inertia\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. What did you analysed from the scores and further steps:\n",
        "\n"
      ],
      "metadata": {
        "id": "sx4kpdMjUhRv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the plots, selecting **three** clusters seems like a reasonable choice for the K-Means algorithm. Let's analyze why:\n",
        "\n",
        "**Silhouette Score:** This score measures how similar an object is to its own cluster compared to other clusters. The higher the silhouette score, the more appropriately the object has been classified. The silhouette score is highest at two clusters and second highest at **three** clusters, it suggests that two or **three** is the optimal number of clusters where the objects have the most similarity to their own cluster and the least similarity to other clusters.\n",
        "\n",
        "**Elbow Method:** This method looks at the inertia or within-cluster sum of squares. The 'elbow' point in the plot is where the rate of decrease sharply changes, indicating that adding more clusters beyond this point does not significantly improve the fitting of the data. If this 'elbow' is observed at **three** clusters, it implies that increasing the number of clusters beyond **three** results in diminishing returns in terms of variance explained.\n",
        "\n",
        "**Given these indications, proceeding with **three** clusters for K-Means is justified. After finalizing the clusters with K-Means, we can compare the results with Hierarchical Clustering and DBSCAN to validate our choice.**"
      ],
      "metadata": {
        "id": "DFNC0ZkdUqBS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2 - **Hierarchical Clustering**"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "By comparing the results of Hierarchical Clustering (via a dendrogram) with the K-Means clustering graphs, we can gain a deeper understanding of the optimal clustering structure for the dataset.\n",
        "\n",
        "Let's proceed with Hierarchical Clustering and generate a dendrogram. After obtaining the dendrogram, we'll compare it with the results from K-Means to see if they suggest a similar number of clusters."
      ],
      "metadata": {
        "id": "uINeIIm3bVAk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 2 Implementation\n",
        "\n",
        "# Performing Hierarchical Clustering\n",
        "\n",
        "# Fit the Algorithm\n",
        "linked = linkage(data_clustering, method='ward')"
      ],
      "metadata": {
        "id": "m7-h4p19c71Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "\n",
        "# Creating a dendrogram\n",
        "plt.figure(figsize=(10, 6))\n",
        "dendrogram(linked, orientation='top', distance_sort='descending', show_leaf_counts=True)\n",
        "plt.title('Hierarchical Clustering Dendrogram')\n",
        "plt.xlabel('Cluster Size')\n",
        "plt.ylabel('Distance')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. What did you analysed from the scores and further steps:\n",
        "\n"
      ],
      "metadata": {
        "id": "YxqiVTYjnQqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analyzing the dendrogram can help us determine the number of clusters. Typically, we look for the largest vertical distance that doesn't intersect any horizontal lines, as cutting the dendrogram at this height would yield distinct clusters.\n",
        "\n",
        "After comparing the results of the K-Means clustering analysis with the Hierarchical Clustering dendrogram, here are my observations:\n",
        "\n",
        "**K-Means Clustering:**\n",
        "\n",
        "The Silhouette Scores suggested that the clusters are reasonably well-defined at three clusters, as indicated by a peak in the silhouette score.\n",
        "The Elbow Method showed a bend around three clusters, suggesting that this might be an optimal point where adding more clusters doesn't significantly improve the fit.\n",
        "\n",
        "**Hierarchical Clustering (Dendrogram):**\n",
        "\n",
        "The **dendrogram** provides a visual representation of the cluster formation process. If we look for the largest vertical distance that doesn't intersect any horizontal lines, we can observe a significant gap that suggests around **two or three** main clusters.\n",
        "\n",
        "**Comparison and Conclusion:**\n",
        "\n",
        "Both methods seem to align well, suggesting that the dataset naturally forms around **two to three** distinct clusters.\n",
        "\n",
        "The slight difference (**two versus three clusters**) can be attributed to the methodologies of the clustering algorithms. K-Means is centroid-based and optimizes cluster cohesion, while Hierarchical Clustering is connectivity-based and considers the entire data structure."
      ],
      "metadata": {
        "id": "mQXZ9BH5nR2u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Next Step:\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "y6kDV5mppJru"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Further Validation with DBSCAN:**\n",
        "\n",
        "To reinforce our decision, we can use DBSCAN (Density-Based Spatial Clustering of Applications with Noise) as a final step. DBSCAN does not require specifying the number of clusters beforehand and can handle outliers effectively.\n",
        "If DBSCAN indicates a clustering structure that aligns closely with the **two-three**-cluster solution from K-Means and Hierarchical Clustering, it will further validate our choice."
      ],
      "metadata": {
        "id": "cBIOEc2TkvO5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3 - DBSCAN"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Finding best parameters for DBSCAN\n",
        "\n",
        "# Define a range of values for 'eps' and 'min_samples'\n",
        "eps_values = [0.3, 0.5, 0.7, 1.0]\n",
        "min_samples_values = [3, 5, 7, 10]\n",
        "\n",
        "best_silhouette = -1\n",
        "best_eps = None\n",
        "best_min_samples = None\n",
        "best_cluster_count = None\n",
        "\n",
        "for eps in eps_values:\n",
        "    for min_samples in min_samples_values:\n",
        "        # Applying DBSCAN with current eps and min_samples\n",
        "        dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
        "        labels = dbscan.fit_predict(data_clustering)\n",
        "\n",
        "        # Counting the number of clusters (ignoring noise points)\n",
        "        n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
        "\n",
        "        # Calculating silhouette score only if more than 1 cluster is found\n",
        "        if n_clusters > 1:\n",
        "            silhouette = silhouette_score(data_clustering, labels)\n",
        "            print(f'eps: {eps}, min_samples: {min_samples}, clusters: {n_clusters}, silhouette: {silhouette}')\n",
        "\n",
        "            # Update best parameters if silhouette score is improved\n",
        "            if silhouette > best_silhouette:\n",
        "                best_silhouette = silhouette\n",
        "                best_eps = eps\n",
        "                best_min_samples = min_samples\n",
        "                best_cluster_count = n_clusters\n",
        "\n",
        "print(f'Best parameters - eps: {best_eps}, min_samples: {best_min_samples}, clusters: {best_cluster_count}, silhouette: {best_silhouette}')\n"
      ],
      "metadata": {
        "id": "qrosGF1Eye_C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation\n",
        "\n",
        "# Applying DBSCAN clustering for the best parameter obtained above\n",
        "dbscan = DBSCAN(eps=1, min_samples=3)\n",
        "clusters_dbscan = dbscan.fit_predict(data_clustering)\n",
        "\n",
        "# Counting the number of clusters and noise points\n",
        "n_clusters_dbscan = len(set(clusters_dbscan)) - (1 if -1 in clusters_dbscan else 0)\n",
        "n_noise_dbscan = list(clusters_dbscan).count(-1)\n",
        "\n",
        "n_clusters_dbscan, n_noise_dbscan, set(clusters_dbscan)"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dimensionality Reduction For Visualization (DBSCAN)"
      ],
      "metadata": {
        "id": "3rbFeII0lNs3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Performing PCA for dimensionality reduction for visualization\n",
        "pca = PCA(n_components=2)\n",
        "principal_components = pca.fit_transform(data_clustering)\n",
        "\n",
        "# Creating a DataFrame for the PCA results\n",
        "pca_df = pd.DataFrame(data=principal_components, columns=['PC1', 'PC2'])\n",
        "pca_df['DBSCAN Cluster'] = clusters_dbscan\n",
        "\n",
        "# Plotting the DBSCAN clusters\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.scatterplot(x='PC1', y='PC2', hue='DBSCAN Cluster', data=pca_df, palette='Set1', alpha=0.8)\n",
        "plt.title('DBSCAN Clusters Visualization with PCA')\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.legend(title='DBSCAN Cluster')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "x3V1sY7n46N-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Number of Clusters:** DBSCAN identified **5 distinct clusters** in the dataset.\n",
        "\n",
        "**Noise Points:** There are 76 noise points. These are points that didn't fit well into any cluster, as per the DBSCAN algorithm.\n",
        "\n",
        "**Cluster Labels:** The clusters are labeled as {-1, 0, 1, 2, 3, 4}, where -1 represents noise points."
      ],
      "metadata": {
        "id": "loDeLnFa0AUR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Analysis`"
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   **DBSCAN'**s identification of **5 clusters** slightly differs from our earlier observation of **2 to 3** clusters using **K-Means and Hierarchical Clustering**. This difference could be due to DBSCAN's density-based nature, which can form clusters of varying densities that centroid-based methods (like K-Means) might not detect.\n",
        "\n",
        "\n",
        "\n",
        "*   **DBSCAN is capturing patterns not well and therefore is not able to form clusters in a way that significantly divide them for this type of data**\n",
        "\n",
        "\n",
        "\n",
        "*   The presence of a significant number of noise points indicates that there might be some data points that don't conform well to any specific cluster. This is a characteristic aspect of DBSCAN and can be valuable in identifying outliers.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rNKaZr-P0sF_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Next Step"
      ],
      "metadata": {
        "id": "xHwhoYQK1QLV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, after analysing the suggestions and behaviours of all 3 algorithms, we will choose **3 clusters to be formed**.\n",
        "\n",
        "Backings:\n",
        "\n",
        "*   **K-Means** suggested **2-3 clusters** as inferred from the algorithm and charts.\n",
        "*   **Hierarchical CLustering** suggested **2-3 clusters** as inferred from the algorithm and charts.\n",
        "*   **DBSCAN** suggested **5 clusters** as inferred from the algorithm and charts and analysis. But, when we visualized the clusters it shows that it is not able to make clusters well for this type of data\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CFivWnKC1VUp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Best Model (KMeans) Dimensionality Reduction For Visualization"
      ],
      "metadata": {
        "id": "G9_CtOqvla0d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assigning data points to clusters using K-Means with 3 clusters\n",
        "kmeans_final = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
        "cluster_labels = kmeans_final.fit_predict(data_clustering)\n",
        "\n",
        "# Applying PCA for dimensionality reduction for visualization purposes\n",
        "pca = PCA(n_components=2)\n",
        "principal_components = pca.fit_transform(data_clustering)\n",
        "\n",
        "# Creating a DataFrame for the PCA results\n",
        "pca_df = pd.DataFrame(data=principal_components, columns=['PC1', 'PC2'])\n",
        "pca_df['Cluster'] = cluster_labels\n",
        "\n",
        "# Plotting the clusters\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.scatterplot(x='PC1', y='PC2', hue='Cluster', data=pca_df, palette='Set1', alpha=0.8)\n",
        "plt.title('Clusters Visualization with PCA')\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.legend(title='Cluster')\n",
        "\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "Q7EmKsLg3EbR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The clusters formed by the K-Means algorithm with **three clusters** are visualized above using PCA (Principal Component Analysis) for dimensionality reduction. In this 2D plot, each point represents a data instance, and the colors represent the different clusters."
      ],
      "metadata": {
        "id": "xSVJODgd8ceo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assigning data points to clusters using K-Means with 3 clusters\n",
        "kmeans_final = KMeans(n_clusters=3, random_state=42,n_init=10)\n",
        "cluster_labels = kmeans_final.fit_predict(data_clustering)\n",
        "\n",
        "# Applying PCA for dimensionality reduction for visualization purposes\n",
        "pca = PCA(n_components=3)  # Use 3 components for 3D visualization\n",
        "principal_components = pca.fit_transform(data_clustering)\n",
        "\n",
        "# Creating a DataFrame for the PCA results\n",
        "pca_df = pd.DataFrame(data=principal_components, columns=['PC1', 'PC2', 'PC3'])\n",
        "pca_df['Cluster'] = cluster_labels\n",
        "\n",
        "# Create an interactive 3D scatter plot using Plotly\n",
        "fig = px.scatter_3d(pca_df, x='PC1', y='PC2', z='PC3', color='Cluster', symbol='Cluster')\n",
        "\n",
        "# Customize the plot layout\n",
        "fig.update_layout(\n",
        "    title='K-Means Clusters Visualization with PCA (3D)',\n",
        "    scene=dict(\n",
        "        xaxis_title='Principal Component 1',\n",
        "        yaxis_title='Principal Component 2',\n",
        "        zaxis_title='Principal Component 3'\n",
        "    )\n",
        ")\n",
        "\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "sW379QB063Ol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The clusters formed by the K-Means algorithm with **three clusters** are visualized above using PCA (Principal Component Analysis) for dimensionality reduction. In this 3D inetractive plot, each point represents a data instance, and the colors represent the different clusters."
      ],
      "metadata": {
        "id": "V07SrvnI6kS3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Cluster Characteristics Analysis***"
      ],
      "metadata": {
        "id": "EwqivJA2BYE2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To understand the behaviors and characteristics of each cluster, we will analyze the features within each cluster. This involves examining the central tendencies (like means or medians) and distributions of features within each cluster. This analysis can provide insights into what distinguishes each cluster from the others."
      ],
      "metadata": {
        "id": "cYg3U_KPB66u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding the cluster labels to the original dataset for analysis\n",
        "data_with_clusters = data_clustering.copy()\n",
        "data_with_clusters['Cluster'] = cluster_labels\n",
        "\n",
        "# Analyzing each cluster\n",
        "cluster_characteristics = {}\n",
        "for cluster in range(3):\n",
        "    cluster_data = data_with_clusters[data_with_clusters['Cluster'] == cluster]\n",
        "    cluster_characteristics[cluster] = cluster_data.describe().iloc[1, :]  # Using mean values for summary\n",
        "\n",
        "# Creating a DataFrame for easier visualization of cluster characteristics\n",
        "cluster_characteristics_df = pd.DataFrame(cluster_characteristics)\n",
        "\n",
        "# Create a boolean mask for rows where the row name is 'Cost' or starts with 'Primary'\n",
        "mask = (cluster_characteristics_df.index == 'Cost') | cluster_characteristics_df.index.str.startswith('Primary')\n",
        "\n",
        "# Use the mask to filter the DataFrame\n",
        "cluster_characteristics_df = cluster_characteristics_df[mask]\n",
        "\n",
        "# Print the filtered DataFrame\n",
        "cluster_characteristics_df\n",
        "\n"
      ],
      "metadata": {
        "id": "ZYDrvAqXCEDl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   **Cluster 0:**\n",
        "    \n",
        "    *   **Cost:** Lowest among the clusters. This suggests that restaurants in Cluster 0 are the most budget-friendly options in the dataset.\n",
        "    *   **Cuisine Preferences:** Dominated by 'North Indian' cuisine. Other notable cuisines in this cluster include 'Bakery' and 'Fast Food'. There is less emphasis on other cuisines compared to other clusters, indicating a specific focus on these cuisines.\n",
        "*   **Cluster 1:**\n",
        "    \n",
        "    *   **Cost:** Moderate among the clusters. This indicates that Cluster 1 restaurants are priced between the high-end and budget-friendly options.\n",
        "    *   **Cuisine Preferences:** Dominated by 'North Indian' cuisine. Other notable cuisines include 'Chinese' and 'Cafe'. This cluster shows a balanced mix of cuisines, neither too specialized nor too diverse.\n",
        "*   **Cluster 2:**\n",
        "    \n",
        "    *   **Cost:** Highest among the clusters. This indicates that Cluster 2 includes the most upscale and premium dining options in the dataset.\n",
        "    *   **Cuisine Preferences:** Dominated by 'North Indian' cuisine. Other notable cuisines include 'Asian' and 'European'. This cluster seems to cater to a market looking for diverse and international dining experiences, with a focus on upscale options."
      ],
      "metadata": {
        "id": "RcQqnsEGF0Sz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interpretation:**\n",
        "\n",
        "*   **Cluster 0** represents the most budget-friendly dining options, focusing primarily on North Indian cuisine, supplemented by Bakery and Fast Food options. This cluster caters to cost-conscious diners looking for popular and familiar choices.\n",
        "    \n",
        "*   **Cluster 1** offers a moderate dining experience with a slight emphasis on North Indian cuisine, alongside a balanced mix of Chinese and Cafe options. This cluster caters to customers seeking a mid-range dining experience with some variety.\n",
        "    \n",
        "*   **Cluster 2** is characterized by upscale, premium dining options, predominantly featuring North Indian cuisine, while also offering a mix of Asian and European cuisines. This cluster is likely to attract customers seeking high-end dining experiences with a variety of international flavors.\n",
        "    \n",
        "\n",
        "Each cluster reflects a distinct segment of the dining market, providing insights into customer preferences and potential strategies for targeted marketing and restaurant positioning.\n"
      ],
      "metadata": {
        "id": "F69ETYRYJooq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***9.*** ***Future Work (For clustering - Zomato Restaurant Clustering)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use the **joblib** library, which is often preferred for saving larger NumPy arrays, to save the **K-Means model**."
      ],
      "metadata": {
        "id": "smaMVORFLD6f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the File\n",
        "\n",
        "# Save the KMeans model as a joblib file\n",
        "joblib.dump(kmeans_final, 'kmeans_model.joblib')\n",
        "\n",
        "# Indicating that the model has been saved\n",
        "model_saved = \"Model saved as 'kmeans_model.joblib'\"\n",
        "\n",
        "model_saved\n"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the File and predict unseen data.\n",
        "\n",
        "# Creating a sample unseen data point\n",
        "# Note: The number of features in the sample data point should match the number of features used in the model training\n",
        "# Here, we're creating a sample with a random value for each feature used in the model\n",
        "num_features = data_clustering.shape[1]\n",
        "sample_data_point = np.random.rand(1, num_features)\n",
        "\n",
        "# Loading the saved model\n",
        "loaded_model = load('/content/kmeans_model.joblib')\n",
        "\n",
        "feature_names = data_clustering.columns.tolist()\n",
        "\n",
        "# Create a DataFrame for the sample data point with the correct feature names\n",
        "sample_data_point_df = pd.DataFrame(sample_data_point, columns=feature_names)\n",
        "\n",
        "# Predicting the cluster for the unseen data point using the loaded model\n",
        "predicted_cluster = loaded_model.predict(sample_data_point_df)\n",
        "\n",
        "print('Predicted cluster number: ', predicted_cluster[0])"
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**As seen, we have successfully saved and loaded the model again and predicted sample datapoint in a cluster.**"
      ],
      "metadata": {
        "id": "2vvCpDKARTgH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion (For clustering - Zomato Restaurant Clustering)**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusion of the Analysis:**\n",
        "\n",
        "**Cluster Identification:**\n",
        "\n",
        "We successfully segmented the dataset of Zomato restaurants into three distinct clusters using K-Means clustering, validated by Hierarchical Clustering and DBSCAN.\n",
        "\n",
        "These clusters represent varying dining experiences: budget-friendly options, upscale dining, mid-range variety, and casual/moderately priced dining.\n",
        "\n",
        "\n",
        "**Market Segmentation Insight:**\n",
        "\n",
        "The analysis provides valuable insights into different market segments in the restaurant industry. Each cluster represents a unique combination of cost and cuisine type, catering to different customer preferences and dining occasions.\n",
        "\n",
        "This segmentation can assist stakeholders in understanding the competitive landscape, identifying market gaps, and tailoring marketing strategies to target specific customer segments effectively."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing (For sentiment analysis - Zomato Reviews)***"
      ],
      "metadata": {
        "id": "xiMSeoLHLRJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "dw9CoJ76LRKG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "\n",
        "# We have already handled missing values during data wrangling.\n",
        "\n",
        "# Verifying again\n",
        "\n",
        "reviews_df.isnull().sum()"
      ],
      "metadata": {
        "id": "PIE2fG-dLRKG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "s4XehvuiLRKH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "\n",
        "# There is only one column that can have outlier.\n",
        "\n",
        "# Normally the ratings are between 0 to 5\n",
        "\n",
        "sns.boxplot(y=reviews_df['Rating'])\n",
        "plt.title('Ratings Distribution')\n",
        "plt.ylabel('Ratings')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "2IxUWvfwLRKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ratings are distributed from 1 to 5 that is normal and also it doesn't have any outlier"
      ],
      "metadata": {
        "id": "IhQYd6NhUrxW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reviews_df.to_csv('reviews_df.csv')"
      ],
      "metadata": {
        "id": "xV2VuTJIu_Tt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def expand_contractions(text):\n",
        "    expanded_text = contractions.fix(text)\n",
        "    return expanded_text\n",
        "\n",
        "# Applying the function to your DataFrame\n",
        "reviews_df['Expanded_Review'] = reviews_df['Review'].apply(expand_contractions)\n"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower Casing\n",
        "\n",
        "# Convert text to lowercase\n",
        "reviews_df['Lowercase_Review'] = reviews_df['Expanded_Review'].str.lower()\n"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations\n",
        "\n",
        "# Remove punctuation\n",
        "reviews_df['No_Punctuation_Review'] = reviews_df['Lowercase_Review'].str.replace('[^\\w\\s]', '')\n"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URLs & Remove words and digits contain digits\n",
        "\n",
        "# Function to remove URLs and words containing digits\n",
        "def remove_urls_and_digits(text):\n",
        "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
        "    text = re.sub(r'\\w*\\d\\w*', '', text) # Remove words containing digits\n",
        "    return text\n",
        "\n",
        "reviews_df['Cleaned_Review'] = reviews_df['No_Punctuation_Review'].apply(remove_urls_and_digits)\n"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stopwords\n",
        "\n",
        "# Function to remove stopwords\n",
        "def remove_stopwords(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    return ' '.join([word for word in tokens if word not in stopwords.words('english')])\n",
        "\n",
        "reviews_df['Stopwords_Removed_Review'] = reviews_df['Cleaned_Review'].apply(remove_stopwords)\n"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove White spaces\n",
        "\n",
        "# Removing extra white spaces\n",
        "reviews_df['Whitespace_Removed_Review'] = reviews_df['Stopwords_Removed_Review'].str.strip().replace('\\s+', ' ', regex=True)\n"
      ],
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Rephrase Text"
      ],
      "metadata": {
        "id": "c49ITxTc407N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rephrase Text\n",
        "\n",
        "\n",
        "# Rephrasing in sentiment analysis is not necessary.\n",
        "# Rephrasing could potentially alter the original sentiment of the review, leading to inaccurate analysis.\n",
        "\n"
      ],
      "metadata": {
        "id": "foqY80Qu48N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "\n",
        "# Already done in the stopword removal step"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def lemmatize_text(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    return ' '.join([lemmatizer.lemmatize(word) for word in tokens])\n",
        "\n",
        "reviews_df['Normalized_Review'] = reviews_df['Stopwords_Removed_Review'].apply(lemmatize_text)"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used lemmatization over stemming because it generally provides better results for sentiment analysis by reducing words to their meaningful base forms."
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Part of speech tagging"
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# POS Tagging\n",
        "\n",
        "# This step requires tokenized text\n",
        "def pos_tagging(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    return nltk.pos_tag(tokens)\n",
        "\n",
        "reviews_df['POS_Tagged_Review'] = reviews_df['Normalized_Review'].apply(pos_tagging)\n"
      ],
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing Text\n",
        "\n",
        "# Code for vectorizing text\n",
        "# The choice of vectorization (TF-IDF, CountVectorizer, Word2Vec, etc.) depends on the model and context.\n",
        "# Example with TF-IDF:\n",
        "tfidf = TfidfVectorizer()\n",
        "X_tfidf = tfidf.fit_transform(reviews_df['Normalized_Review'])\n"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The choice between TF-IDF, CountVectorizer, Word2Vec, etc., depends on the specific requirements of the sentiment analysis model. TF-IDF is a common choice for its balance between word frequency and its relevance across documents."
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "bzJc4okfLRKN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "wDOtj-bp40T1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding a feature of 'Review_Length' that could be important factor for review sentiment\n",
        "\n",
        "reviews_df['Review_Length'] = reviews_df['Review'].apply(len)"
      ],
      "metadata": {
        "id": "cHQgBp8txdjc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "L5mhlSvWLRKO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We did all the above steps and added one column for each step just for demonstration purpose.\n",
        "\n",
        "# We will use last filtered column that has all the step applied that is 'Normalized_Review'.\n",
        "\n",
        "reviews_df.head()"
      ],
      "metadata": {
        "id": "e2OdBPb0thWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We did all the above steps from the template and added one column for each step just for demonstration purpose."
      ],
      "metadata": {
        "id": "6Bfg9HTss42I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use **last filtered column that has all the step applied that is 'Normalized_Review'.**\n",
        "\n",
        "**We wont use POS tagged data for the following reason:**\n",
        "\n",
        "Here's why POS tagging might or might not be used directly in basic sentiment analysis models:\n",
        "\n",
        "**Focus on Text Content:** Basic sentiment analysis often primarily focuses on the content of the text (i.e., the words themselves and their order). For many models, especially those that use bag-of-words approaches like TF-IDF, the syntactic information provided by POS tags may not significantly impact performance.\n",
        "\n",
        "**Model Complexity:** Incorporating POS tags directly into models like RandomForestClassifier (used with TF-IDF) can increase complexity without necessarily improving performance for basic sentiment analysis tasks. These models don't inherently utilize the syntactic structure of the text.\n",
        "\n",
        "**Advanced NLP Tasks:** POS tags are more commonly used in advanced NLP tasks that require understanding the grammatical structure of sentences, such as dependency parsing, named entity recognition, or certain types of text classification where the syntactic context of words is crucial.\n",
        "\n",
        "**Deep Learning Approaches:** In deep learning models like LSTM or BERT, which understand the context and sequence of words, POS tags might be implicitly considered by the model's architecture without needing explicit POS tagging."
      ],
      "metadata": {
        "id": "WW-RIHJts9SP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dropping unnecessary columns\n",
        "columns_to_drop = ['Reviewer', 'Metadata', 'Time', 'Pictures', 'Expanded_Review',\n",
        "                   'Lowercase_Review', 'No_Punctuation_Review', 'Cleaned_Review',\n",
        "                   'Stopwords_Removed_Review', 'Whitespace_Removed_Review', 'POS_Tagged_Review']\n",
        "reviews_df = reviews_df.drop(columns=columns_to_drop)\n",
        "\n",
        "# Your DataFrame now contains 'Restaurant', 'Review', 'Rating', 'Normalized_Review', and 'Review_Length'\n"
      ],
      "metadata": {
        "id": "n3oSrRzd0aUG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As other columns were added for showing the inclusion of all steps, we will now drop them and keep the filtered column with all preprocessing applied that is **'Normalized_Review'**\n",
        "\n",
        "Also, we will keep **'Restaurant'**, **'Rating'** , **'Review'**, **'Review_Length'**\n",
        "\n",
        "Among these all columns, we are keeping **'Restaurant'** for further analysis after training the model (Like which resturant is performing well or have more positive or negative reviews based on predicted sentiments) and **wont be used in model training**\n",
        "\n",
        "We are also keeping 'Review' for comparision with 'Normalized_Review' and **wont be used in model training**\n",
        "\n",
        "**For model training and prediction** we will use **'Rating'** , **'Normalized_Review'** and **'Review_Length'**, where **'Rating' will be used to label some data.**\n",
        "\n"
      ],
      "metadata": {
        "id": "PBZI33ex7T6R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?\n",
        "\n",
        "Explained above"
      ],
      "metadata": {
        "id": "FLqVyZohLRKO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "Ni7XN1W6LRKP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding 'Review_Length' as a feature\n",
        "X_combined = sp.hstack((X_tfidf, reviews_df[['Review_Length']].values.astype(float)), format='csr')\n"
      ],
      "metadata": {
        "id": "fQVEp61C02Sp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have combined TF-IDF Features with Review Length"
      ],
      "metadata": {
        "id": "ganlAbAcCmdT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming ratings > 3 are positive (1) and <= 3 are negative (0)\n",
        "y = (reviews_df['Rating'] > 3).astype(int)\n"
      ],
      "metadata": {
        "id": "hcwUHZkt07jT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Considering ratings that are greater than 3 as positive else negative to label the data for model training."
      ],
      "metadata": {
        "id": "A0u-L7qdDaEw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "e6HZ-4gfmzJG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1 - Logistic Regression"
      ],
      "metadata": {
        "id": "XpEF0KpVEdmw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "-qAbB-_9G1_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic Regression model\n",
        "log_reg = LogisticRegression(max_iter=1000, random_state=42)\n",
        "log_reg.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "fpDN3mpo3RBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "cW89xG-rEdm7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predictions and Evaluation\n",
        "y_pred_log_reg = log_reg.predict(X_test)\n",
        "print(\"Logistic Regression Evaluation\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_log_reg))\n",
        "print(classification_report(y_test, y_pred_log_reg))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_log_reg))"
      ],
      "metadata": {
        "id": "orWiK2WBGSkX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2 - Random Forest"
      ],
      "metadata": {
        "id": "2Bs-raksFHQz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 2 Implementation\n",
        "\n",
        "# Train a RandomForestClassifier\n",
        "clf = RandomForestClassifier(random_state=42)\n",
        "clf.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "WC9lm0GR0-lq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "UvHk5II3GOVP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "\n",
        "# Making predictions on the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Evaluating the model\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "Tdfqv4IJ1UQf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3 - XGBoost"
      ],
      "metadata": {
        "id": "P-LMbiPrI_4-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# XGBoost model\n",
        "xgb_clf = xgb.XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
        "xgb_clf.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "KjC4xAO23Zk8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "rgxrRYrqP10I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predictions and Evaluation\n",
        "y_pred_xgb = xgb_clf.predict(X_test)\n",
        "print(\"\\nXGBoost Evaluation\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_xgb))\n",
        "print(classification_report(y_test, y_pred_xgb))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_xgb))"
      ],
      "metadata": {
        "id": "_rhl8ypYP6uc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 4"
      ],
      "metadata": {
        "id": "R5PQ9JsaPlMV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradient Boosting model\n",
        "gb_clf = GradientBoostingClassifier(random_state=42)\n",
        "gb_clf.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "TprafUUg3iFJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "qEplumiwQN20"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predictions and Evaluation\n",
        "y_pred_gb = gb_clf.predict(X_test)\n",
        "print(\"\\nGradient Boosting Evaluation\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_gb))\n",
        "print(classification_report(y_test, y_pred_gb))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_gb))"
      ],
      "metadata": {
        "id": "vF6OEP3cQRDY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Performance Summary:\n",
        "\n",
        "**1\\. Logistic Regression**\n",
        "\n",
        "*   **Accuracy:** 87.8% - This is the highest among all models.\n",
        "*   **Precision:** Good balance, slightly higher for class 1 (positive sentiment).\n",
        "*   **Recall:** Better at identifying class 1 (positive sentiment), with a notable difference compared to class 0 (negative sentiment).\n",
        "*   **F1-Score:** High for both classes, particularly for class 1, indicating a strong overall performance.\n",
        "\n",
        "**2\\. Random Forest**\n",
        "\n",
        "*   **Accuracy:** 85.94% - Slightly lower than Logistic Regression.\n",
        "*   **Precision and Recall:** Comparatively lower than Logistic Regression, especially recall for class 0 (negative sentiment) is significantly lower.\n",
        "*   **F1-Score:** Fair for both classes but lower than Logistic Regression, especially for class 0.\n",
        "\n",
        "**3\\. XGBoost**\n",
        "\n",
        "*   **Accuracy:** 86.39% - Close to Logistic Regression, but slightly lower.\n",
        "*   **Precision and Recall:** Balanced, with a marginal preference for class 1. Recall for class 0 is better than Random Forest but not as good as Logistic Regression.\n",
        "*   **F1-Score:** High for both classes, indicating effective performance, particularly for class 1.\n",
        "\n",
        "**4\\. Gradient Boosting**\n",
        "\n",
        "*   **Accuracy:** 84.08% - The lowest among the models.\n",
        "*   **Precision and Recall:** Precision is relatively balanced, but recall is notably lower for class 0, affecting its overall performance.\n",
        "*   **F1-Score:** Lower for class 0, indicating challenges in identifying negative sentiments as effectively as other models."
      ],
      "metadata": {
        "id": "aQokg3iwSOc-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analysis and Recommendations\n",
        "\n",
        "*   **Logistic Regression** exhibits the highest overall accuracy and demonstrates a commendable balance between precision and recall for both classes. It seems to be the best-performing model among those evaluated, especially in identifying positive sentiments with high accuracy.\n",
        "    \n",
        "*   **Random Forest and XGBoost** show competitive performance, but Logistic Regression has a slight edge in overall accuracy and balance between precision and recall.\n",
        "    \n",
        "*   **Gradient Boosting** displays lower performance, particularly in terms of recall for negative sentiments (class 0), indicating a struggle in accurately identifying negative reviews compared to other models.\n",
        "    \n",
        "\n",
        "Considering these results, **Logistic Regression** appears to be the most suitable model for your analysis task. It maintains a high level of precision and recall across both positive and negative sentiments, making it an effective choice for balanced sentiment analysis.\n"
      ],
      "metadata": {
        "id": "m3S2h2irTAXW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameter Tuning Of Logistic Regression(Best Model)"
      ],
      "metadata": {
        "id": "3RVXUxF_UYyg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters to tune\n",
        "param_grid = {\n",
        "    'C': [0.001, 0.01, 0.1, 1, 10, 100],  # Values for C\n",
        "    'penalty': ['l1', 'l2'],  # Regularization types\n",
        "    'solver': ['liblinear']  # Solver liblinear works well with l1 and l2\n",
        "}\n",
        "\n",
        "# Create a LogisticRegression model\n",
        "log_reg = LogisticRegression()\n",
        "\n",
        "# Grid Search with Cross-Validation\n",
        "grid_search = GridSearchCV(estimator=log_reg, param_grid=param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "# Fit the model on training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters\n",
        "best_parameters = grid_search.best_params_\n",
        "print(\"Best Parameters for Logistic Regression:\", best_parameters)\n"
      ],
      "metadata": {
        "id": "dgA3u31STZkt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training Logistic Regression again with the best parameters found"
      ],
      "metadata": {
        "id": "7z6mcENrUmqI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic Regression model\n",
        "log_reg = LogisticRegression(max_iter=1000, random_state=42, C = 1, penalty='l2', solver = 'liblinear' )\n",
        "log_reg.fit(X_train, y_train)\n",
        "\n",
        "# Predictions and Evaluation\n",
        "y_pred_log_reg = log_reg.predict(X_test)\n",
        "print(\"Logistic Regression Evaluation\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_log_reg))\n",
        "print(classification_report(y_test, y_pred_log_reg))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_log_reg))\n"
      ],
      "metadata": {
        "id": "7FQlMwFqTq3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The hyperparameter tuning didn't further improved model's performance other than f1-score for class 0,but still it maintains a high level of accuracy and a good balance between precision and recall.\n",
        "\n",
        "Given the results, this model seems well-suited for your sentiment analysis task."
      ],
      "metadata": {
        "id": "Y_q3wI2qVmrj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (For sentiment analysis - Zomato Reviews)***"
      ],
      "metadata": {
        "id": "rpR2M3FrYFXv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the Logistic Regression model as a joblib file\n",
        "joblib.dump(log_reg, 'logistic_regression_model_sentiment.joblib')\n",
        "\n",
        "# Indicating that the model has been saved\n",
        "model_saved = \"Model saved as 'logistic_regression_model_sentiment.joblib'\"\n",
        "print(model_saved)\n"
      ],
      "metadata": {
        "id": "1lrvHlgfYQ7I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the saved model\n",
        "loaded_model = joblib.load('logistic_regression_model_sentiment.joblib')\n",
        "\n",
        "# Define two sample texts\n",
        "sample_texts = [\"The food was absolutely wonderful, from preparation to presentation, very pleasing.\",\n",
        "                \"I was very disappointed with the food. It was tasteless and not worth the money.\"]\n",
        "\n",
        "sample_tfidf = tfidf.transform(sample_texts)\n",
        "\n",
        "sample_review_lengths = [len(text) for text in sample_texts]\n",
        "\n",
        "# Combine TF-IDF features with 'Review_Length' for each sample text\n",
        "sample_features = [sp.hstack((sample_tfidf[i], np.array([[length]])), format='csr') for i, length in enumerate(sample_review_lengths)]\n",
        "\n",
        "# Predicting the sentiment for each sample data point using the loaded model\n",
        "predicted_sentiments = [loaded_model.predict(feature) for feature in sample_features]\n",
        "\n",
        "# Output the predictions\n",
        "predicted_sentiment_labels = ['Positive' if sentiment[0] == 1 else 'Negative' for sentiment in predicted_sentiments]\n",
        "\n",
        "# Print predictions for each text\n",
        "for text, sentiment in zip(sample_texts, predicted_sentiment_labels):\n",
        "    print(f\"Review: \\\"{text}\\\" \\nPredicted Sentiment: {sentiment}\\n\")\n"
      ],
      "metadata": {
        "id": "ox-pqY5yYkTz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion (For sentiment analysis - Zomato Reviews)**"
      ],
      "metadata": {
        "id": "iqAn72C0nzVh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After implementing and evaluating four different machine learning models for sentiment analysis, Logistic Regression emerged as the most effective. This conclusion is drawn based on several key aspects:\n",
        "\n",
        "1.  **Performance Metrics**:\n",
        "    \n",
        "    *   **Logistic Regression** exhibited the highest accuracy (87.80%) among all models, indicating its superior overall predictive performance.\n",
        "    *   It demonstrated a well-balanced precision and recall across both sentiment classes, which is crucial for a reliable sentiment analysis model. A high F1-score, particularly for positive sentiments, further underscores its efficiency.\n",
        "2.  **Comparison with Other Models**:\n",
        "    \n",
        "    *   **Random Forest** and **XGBoost** showed commendable performance but were slightly overshadowed by Logistic Regression in terms of accuracy and balance between precision and recall.\n",
        "    *   **Gradient Boosting** lagged in performance, particularly in recalling negative sentiments, making it less favorable for a balanced sentiment analysis task.\n",
        "3.  **Hyperparameter Tuning**:\n",
        "    \n",
        "    *   The hyperparameter tuning of Logistic Regression using GridSearchCV led to the identification of optimal parameters, slightly enhancing the model's efficiency.\n",
        "    *   This fine-tuning confirmed that the chosen parameters (C=1, penalty='l2', solver='liblinear') were near-optimal for this dataset, which slightly improved model performance without overfitting.\n",
        "4.  **Overall Suitability**:\n",
        "    \n",
        "    *   The final tuned Logistic Regression model, with its high accuracy and balanced precision-recall trade-off, is well-suited for sentiment analysis tasks. It effectively addresses the need for accurately identifying both positive and negative sentiments in a dataset.\n",
        "\n",
        "### Recommendation\n",
        "\n",
        "Based on the analysis, it is recommended to use the Logistic Regression model with the identified hyperparameters for sentiment analysis tasks. This model not only provides high accuracy but also ensures a balanced approach to classifying sentiments, which is vital in capturing the nuances of user feedback or reviews."
      ],
      "metadata": {
        "id": "y8a19-a6odW3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "XiD3NXehn8v0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***9.*** ***Extra (For sentiment analysis - Zomato Reviews)***"
      ],
      "metadata": {
        "id": "1DmNDwBFaMgL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now predict sentiment for whole dataset and assign it to column. Then we will try to gain some insights from the whole dataset with predicted sentiment."
      ],
      "metadata": {
        "id": "GpQ0YQmZbaPz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorize the 'Normalized_Review' of the entire dataset\n",
        "X_full_tfidf = tfidf.transform(reviews_df['Normalized_Review'])\n",
        "\n",
        "# Combine with 'Review_Length' if it was a feature in your model\n",
        "X_full_combined = sp.hstack((X_full_tfidf, reviews_df[['Review_Length']].values.astype(float)), format='csr')\n",
        "\n",
        "\n",
        "# Predicting sentiments for the entire dataset\n",
        "predicted_sentiments_full = log_reg.predict(X_full_combined)\n",
        "\n",
        "# Converting numerical predictions to labels ('Positive' or 'Negative')\n",
        "predicted_sentiment_labels = ['Positive' if sentiment == 1 else 'Negative' for sentiment in predicted_sentiments_full]\n",
        "\n",
        "# Adding the sentiment labels to the DataFrame\n",
        "reviews_df['Predicted_Sentiment_Label'] = predicted_sentiment_labels\n",
        "\n",
        "# Now your DataFrame has a 'Predicted_Sentiment_Label' column with the sentiment labels\n"
      ],
      "metadata": {
        "id": "qtzwFXbGbqEj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Picking random 10 sample from reviews_df that has Predicted_Sentiment_Label column now for all the rows\n",
        "\n",
        "reviews_df.sample(10)"
      ],
      "metadata": {
        "id": "gOw3d9gJcDNR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Histogram for Ratings Distribution using the corrected dataset name\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(reviews_df['Rating'], bins=range(1, 7), color='skyblue', edgecolor='black')\n",
        "plt.title('Distribution of Ratings')\n",
        "plt.xlabel('Rating')\n",
        "plt.ylabel('Number of Reviews')\n",
        "plt.xticks(range(1, 6))\n",
        "plt.grid(axis='y', alpha=0.75)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "yEo7oQwCexRW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Insights from the Ratings Distribution:\n",
        "\n",
        "1.  **High Frequency of High Ratings**: The majority of reviews have high ratings (4-5), indicating overall positive experiences.\n",
        "2.  **Scarce Low Ratings**: Ratings below 3 are relatively rare, suggesting few instances of strong dissatisfaction among customers.\n",
        "3.  **Dominance of 5-Star Ratings**: A significant number of reviews are 5-star ratings, suggesting a tendency among customers to give the highest rating for satisfactory experiences.\n"
      ],
      "metadata": {
        "id": "QtWFNkVQgKM2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Distribution of Review Lengths with adjusted scale\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(reviews_df[reviews_df['Review_Length'] <= 1000]['Review_Length'], color='purple', kde=True)\n",
        "plt.title('Distribution of Review Lengths (Up to 1000 Characters)')\n",
        "plt.xlabel('Review Length (Number of Characters)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.grid(axis='y', alpha=0.75)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "NDqk0gLhhUeS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Insights from the Review Lengths Distribution:\n",
        "\n",
        "1.  **Prominent Short Reviews**: The concentration of reviews in the shorter length range is even more evident now, highlighting the preference for brevity.\n",
        "2.  **Peak around 100-200 Characters**: The peak is clearly visible in the 100-200 character range, indicating this is the most common length for reviews.\n",
        "3.  **Rapid Decrease Beyond 200 Characters**: There's a noticeable drop in frequency as the length increases beyond 200 characters, suggesting that longer reviews are less common.\n",
        "4.  **Sparse Longer Reviews**: While there are some reviews longer than 200 characters, they are relatively rare, emphasizing the trend towards shorter reviews.\n",
        "\n",
        "This more focused view reinforces the insight that customers tend to leave concise feedback, with longer reviews being less frequent."
      ],
      "metadata": {
        "id": "4fk3KCQghdvH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating average review length for each sentiment category\n",
        "avg_length_per_sentiment = reviews_df.groupby('Predicted_Sentiment_Label')['Review_Length'].mean().reset_index()\n",
        "\n",
        "# Bar chart for average review length per sentiment\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.barplot(x='Predicted_Sentiment_Label', y='Review_Length', data=avg_length_per_sentiment, palette=['red', 'green'])\n",
        "plt.title('Average Review Length by Sentiment')\n",
        "plt.xlabel('Predicted Sentiment')\n",
        "plt.ylabel('Average Review Length (Characters)')\n",
        "plt.grid(axis='y', alpha=0.75)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "IMfzU7rKiJ9_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Insights from Average Review Length by Sentiment:\n",
        "\n",
        "1.  **Similar Average Lengths**: Both positive and negative sentiment categories have similar average lengths for their reviews. This suggests that the length of a review is not strongly indicative of its sentiment.\n",
        "2.  **Engagement Regardless of Sentiment**: Customers tend to write reviews of similar lengths whether they are expressing positive or negative experiences. This indicates a comparable level of engagement in providing feedback, regardless of sentiment.\n",
        "\n",
        "This analysis provides an insight into how the length of a review relates to the sentiment expressed and suggests that customers are equally detailed in their feedback, whether positive or negative."
      ],
      "metadata": {
        "id": "XhAGUFibim3e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scatter plot for Ratings vs. Review Length\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(data=reviews_df, x='Rating', y='Review_Length', color='blue')\n",
        "plt.title('Ratings vs. Review Length')\n",
        "plt.xlabel('Rating')\n",
        "plt.ylabel('Review Length (Number of Characters)')\n",
        "plt.grid(alpha=0.75)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "REB-LJpUi_zk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Insights from the Ratings vs. Review Length Visualization:\n",
        "\n",
        "1.  **Wide Distribution Across Ratings**: The reviews are spread across all ratings, with no clear concentration in any specific area, indicating a diverse range of experiences.\n",
        "2.  **No Strong Correlation**: There doesn't appear to be a strong correlation between the length of a review and the rating given. Both short and long reviews are present across all rating levels.\n",
        "3.  **Variability in Review Lengths**: There's considerable variability in the lengths of reviews for each rating category. This suggests that customers' decision to write more or less detailed reviews is not strongly influenced by the rating they give.\n",
        "\n",
        "This analysis indicates that the length of a review does not have a clear relationship with the numerical rating, reflecting the individuality of each customer's review style.\n"
      ],
      "metadata": {
        "id": "uS_QcBW9jHwa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the top 10 restaurants with the highest positive sentiments\n",
        "\n",
        "# Counting positive reviews for each restaurant\n",
        "positive_reviews_count = reviews_df[reviews_df['Predicted_Sentiment_Label'] == 'Positive']\\\n",
        "                            .groupby('Restaurant')['Review'].count().reset_index()\n",
        "\n",
        "# Sorting to find the top 10 restaurants with the highest number of positive reviews\n",
        "top_10_positive_sentiment_restaurants = positive_reviews_count.sort_values(by='Review', ascending=False).head(10)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.barplot(x='Review', y='Restaurant', data=top_10_positive_sentiment_restaurants, palette='Greens_r')\n",
        "plt.title('Top 10 Restaurants with the Highest Positive Sentiments')\n",
        "plt.xlabel('Number of Positive Reviews')\n",
        "plt.ylabel('Restaurant')\n",
        "plt.grid(axis='x', alpha=0.75)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "_EMD_LMpjypQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Key Observations:\n",
        "\n",
        "1.  **B-Dubs and AB's - Absolute Barbecues** are leading with the highest number of positive reviews (97 each).\n",
        "2.  **Close Competition**: Several restaurants are closely competing in terms of positive reviews, such as Paradise, 3B's - Buddies, Bar & Barbecue, Flechazo, and The Indi Grill, each with over 90 positive reviews.\n",
        "3.  **High Customer Satisfaction**: These top 10 restaurants demonstrate a strong level of customer satisfaction, as indicated by the high number of positive reviews.\n",
        "\n",
        "This bar chart provides a clear and visually appealing representation of which restaurants are most appreciated by their customers, based on the number of positive reviews."
      ],
      "metadata": {
        "id": "XhKbjndQkObm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a new column to categorize ratings as 'High', 'Medium', or 'Low'\n",
        "def categorize_rating(rating):\n",
        "    if rating >= 4:\n",
        "        return 'High'\n",
        "    elif rating >= 2:\n",
        "        return 'Medium'\n",
        "    else:\n",
        "        return 'Low'\n",
        "\n",
        "reviews_df['Rating_Category'] = reviews_df['Rating'].apply(categorize_rating)\n",
        "\n",
        "# Analyzing the concordance between ratings and sentiment labels\n",
        "rating_sentiment_concordance = reviews_df.groupby(['Rating_Category', 'Predicted_Sentiment_Label']).size().unstack(fill_value=0)\n",
        "\n",
        "# Visualization of concordance between ratings and sentiment labels\n",
        "plt.figure(figsize=(10, 6))\n",
        "rating_sentiment_concordance.plot(kind='bar', stacked=False, color=['red', 'green'], ax=plt.gca())\n",
        "plt.title('Concordance Between Ratings and Sentiment Labels')\n",
        "plt.xlabel('Rating Category')\n",
        "plt.ylabel('Number of Reviews')\n",
        "plt.xticks(rotation=0)\n",
        "plt.legend(title='Predicted Sentiment', labels=['Negative', 'Positive'])\n",
        "plt.grid(axis='y', alpha=0.75)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Qvqfhy_gktCS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Insights from Concordance Between Ratings and Sentiment Labels:\n",
        "\n",
        "1.  **High Ratings and Positive Sentiment Align**: As expected, there's a strong alignment between high ratings and positive sentiment. This indicates that customers who are satisfied tend to give high ratings and positive reviews.\n",
        "2.  **Low Ratings Mostly Negative**: Low ratings are predominantly associated with negative sentiment, which aligns with expectations.\n",
        "3.  **Medium Ratings Show Mixed Sentiments**: In the medium rating category, there is a noticeable presence of both positive and negative sentiments. This suggests a nuanced view in this rating range, where customers might have had mixed experiences.\n",
        "4.  **Minimal Discrepancies**: There are minimal instances where the sentiment seems to contradict the rating category, suggesting that the sentiment analysis aligns well with the numerical ratings in most cases.\n",
        "\n",
        "This visualization provides a clear understanding of how well the sentiment extracted from the text of the reviews aligns with the numerical ratings provided by customers."
      ],
      "metadata": {
        "id": "fZsM7q01k9MA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to create a list of words from reviews\n",
        "def create_word_list(reviews):\n",
        "    words = []\n",
        "    for review in reviews:\n",
        "        words.extend(review.split())\n",
        "    return words\n",
        "\n",
        "# Filtering out non-text data and NaN values from positive reviews\n",
        "positive_reviews = reviews_df[reviews_df['Predicted_Sentiment_Label'] == 'Positive']['Normalized_Review']\n",
        "positive_reviews = positive_reviews.dropna().astype(str)\n",
        "\n",
        "# Creating a list of words from positive reviews\n",
        "positive_words = create_word_list(positive_reviews)\n",
        "\n",
        "# Manually defining a basic set of English stopwords\n",
        "basic_stopwords = set([\"the\", \"and\", \"to\", \"of\", \"a\", \"in\", \"is\", \"it\", \"that\", \"for\", \"on\", \"was\", \"with\", \"as\", \"I\", \"his\", \"he\", \"be\", \"at\", \"one\", \"have\", \"this\", \"from\", \"or\", \"had\", \"by\", \"but\", \"not\", \"what\", \"all\", \"were\", \"we\", \"when\", \"your\", \"can\", \"said\", \"there\", \"use\", \"an\", \"each\", \"which\", \"she\", \"do\", \"how\", \"their\", \"if\", \"will\", \"up\", \"other\", \"about\", \"out\", \"many\", \"then\", \"them\", \"these\", \"so\", \"some\", \"her\", \"would\", \"make\", \"like\", \"him\", \"into\", \"time\", \"has\", \"look\", \"two\", \"more\", \"write\", \"go\", \"see\", \"number\", \"no\", \"way\", \"could\", \"people\", \"my\", \"than\", \"first\", \"water\", \"been\", \"call\", \"who\", \"oil\", \"its\", \"now\", \"find\", \"long\", \"down\", \"day\", \"did\", \"get\", \"come\", \"made\", \"may\", \"part\"])\n",
        "\n",
        "# Removing manually defined stopwords and punctuation from the words list\n",
        "positive_words = [word for word in positive_words if word.lower() not in basic_stopwords and word.isalpha()]\n",
        "\n",
        "# Counting word frequencies\n",
        "positive_word_freq = Counter(positive_words)\n",
        "\n",
        "# Function to create a WordCloud\n",
        "def create_wordcloud(word_freq):\n",
        "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_freq)\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.axis('off')\n",
        "\n",
        "# Creating WordCloud for Positive Review Words\n",
        "create_wordcloud(positive_word_freq)\n",
        "plt.title('Positive Review Words')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "XuBk8k3ZlMTR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Observations from the Positive Review Words Word Cloud:\n",
        "\n",
        "*   **Highlights Key Attributes**: The word cloud emphasizes the words most frequently mentioned in positive reviews. These often reflect key attributes or aspects of the experience that customers particularly appreciated.\n",
        "*   **Understanding Customer Preferences**: Words like \"good\", \"great\", \"best\", \"delicious\", and other positive adjectives suggest a strong level of satisfaction with aspects like food quality, service, or overall experience.\n",
        "*   **Guidance for Business Improvement**: For the restaurant or service being reviewed, these words offer valuable insights into what is working well and should be maintained or further enhanced.\n",
        "\n",
        "This visualization helps to quickly grasp the positive aspects that stand out in customer feedback, providing a guide to what customers value the most."
      ],
      "metadata": {
        "id": "jth5HlIim44C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So with this positive word cloud, let's conclude this project here. "
      ],
      "metadata": {
        "id": "cpls2w8gnu-G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}